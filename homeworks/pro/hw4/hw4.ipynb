{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd279162",
   "metadata": {},
   "source": [
    "## Homework 4: StyleGAN üíáüèª‚Äç‚ôÇÔ∏è\n",
    "\n",
    "–° –º–æ–º–µ–Ω—Ç–∞ —Å–≤–æ–µ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è –≤ 2014 –≥–æ–¥—É –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤ GAN-–∞—Ö —Ä–∞–±–æ—Ç–∞–ª –∫–∞–∫ —á–µ—Ä–Ω—ã–π —è—â–∏–∫. –í —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö —Å–∫—Ä—ã—Ç—ã–π –≤–µ–∫—Ç–æ—Ä $\\mathbf{z}$ –ø–æ–¥–∞–≤–∞–ª—Å—è –≤ –ø–µ—Ä–≤—ã–π —Å–ª–æ–π —Å–µ—Ç–∏, –∏, –ø—Ä–æ—Ö–æ–¥—è —á–µ—Ä–µ–∑ —Ü–µ–ø–æ—á–∫—É —Å–≤–µ—Ä—Ç–æ–∫, –ø—Ä–µ–≤—Ä–∞—â–∞–ª—Å—è –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –•–æ—Ç—è –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä–æ—Å–ª–æ, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å —Å –ø—Ä–æ–±–ª–µ–º–æ–π —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º–æ–π **–∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç–∏** (**entanglement**) —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞. –¢–æ –µ—Å—Ç—å –ø–æ–ø—ã—Ç–∫–∞ –∏–∑–º–µ–Ω–∏—Ç—å –æ–¥–Ω—É —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –ª–∏—Ü–∞ –Ω–µ–∏–∑–±–µ–∂–Ω–æ –≤–ª–µ–∫–ª–∞ –∑–∞ —Å–æ–±–æ–π –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤.\n",
    "\n",
    "–ü—Ä–æ—Ä—ã–≤–æ–º, –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–∏–º –ø–æ—è–≤–ª–µ–Ω–∏—é `StyleGAN`, —Å—Ç–∞–ª–∞ –∏–¥–µ—è **–ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞** (**Progressive Growing**), –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∫–æ–º–∞–Ω–¥–æ–π `NVIDIA` –≤ –º–æ–¥–µ–ª–∏ [ProGAN](https://arxiv.org/pdf/1710.10196). –ò–¥–µ—è –∑–∞–∫–ª—é—á–∞–ª–∞—Å—å –≤ —Ç–æ–º, —á—Ç–æ–±—ã –Ω–∞—á–∏–Ω–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è—è –Ω–æ–≤—ã–µ —Å–ª–æ–∏ –∏ –ø–æ–≤—ã—à–∞—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –∏ –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è $1024\\times 1024$. –û–¥–Ω–∞–∫–æ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –≤ `ProGAN` –æ—Å—Ç–∞–≤–∞–ª–∞—Å—å –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç–∏ —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞.\n",
    "\n",
    "–ê–≤—Ç–æ—Ä—ã [StyleGAN](https://arxiv.org/pdf/1812.04948) –æ—Å—Ç–∞–≤–∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—É—é –∏–¥–µ—é `ProGAN`, –Ω–æ —Ä–∞–¥–∏–∫–∞–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏–ª–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞. –í–¥–æ—Ö–Ω–æ–≤–∏–≤—à–∏—Å—å –º–µ—Ç–æ–¥–∞–º–∏ **–ø–µ—Ä–µ–Ω–æ—Å–∞ —Å—Ç–∏–ª—è** (**Style Transfer**), –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç –ø–æ–¥–∞—á–∏ —Å–∫—Ä—ã—Ç–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ $\\mathbf{z}$ –≤ –ø–µ—Ä–≤—ã–π —Å–ª–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–∞—á–∏–Ω–∞—Ç—å —Å –æ–±—É—á–∞–µ–º–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞,–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å —á–µ—Ä–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—É—é **—Å–µ—Ç—å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è** (**Mapping Network**), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –ª–∞—Ç–µ–Ω—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –∏–∑ –∏—Å–∫—Ä–∏–≤–ª–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ $\\mathcal{Z}$ –≤ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ, —Ä–∞—Å–ø—É—Ç–∞–Ω–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ $\\mathcal{W}$.\n",
    "\n",
    "### –ó–∞–¥–∞–Ω–∏–µ\n",
    "\n",
    "–í–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å `StyleGAN` –∏ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ `CelebA HQ` –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª–∏—Ü.\n",
    "\n",
    "–ó–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–æ–º–∞—à–Ω–µ–≥–æ –∑–∞–¥–∞–Ω–∏—è –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –¥–æ **11 –±–∞–ª–ª–æ–≤**. –î–ª—è —á–∞—Å—Ç–∏ –∑–∞–¥–∞–Ω–∏–π –º—ã –Ω–∞–ø–∏—Å–∞–ª–∏ –¥–ª—è –≤–∞—Å —Å–∫–µ–ª–µ—Ç. –ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤ –Ω–∏—Ö –ø—Ä–æ–ø—É—Å–∫–∏, –≤—ã–¥–µ–ª–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é `...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4744da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import zipfile\n",
    "\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878275a5",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 1: Dataset (0.5 –±–∞–ª–ª–∞)\n",
    "\n",
    "–î–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç `CelebA HQ`, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç $30$ —Ç—ã—Å. –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ª–∏—Ü –∑–Ω–∞–º–µ–Ω–∏—Ç–æ—Å—Ç–µ–π –≤ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ $128\\times 128$ –ø–∏–∫—Å–µ–ª–µ–π.\n",
    "\n",
    "**–í–∞—à–∞ –∑–∞–¥–∞—á–∞**:\n",
    "\n",
    "- –°–∫–∞—á–∞—Ç—å –∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å –∞—Ä—Ö–∏–≤ —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º\n",
    "\n",
    "- –°–æ–∑–¥–∞—Ç—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: \n",
    "    - –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —Ç–µ–Ω–∑–æ—Ä (`ToTensor`).\n",
    "    - —Å–¥–µ–ª–∞—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é (`Normalize`)\n",
    "\n",
    "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏ —Å–æ–∑–¥–∞—Ç—å –∫–ª–∞—Å—Å CelebADataset.\n",
    "\n",
    "- –†–∞–∑–¥–µ–ª–∏—Ç—å —Å–æ–∑–¥–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏ –≤ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–∏ $95\\% / 5\\%$\n",
    "\n",
    "- –°–æ–∑–¥–∞—Ç—å DataLoader'—ã –¥–ª—è –∫–∞–∂–¥–æ–π –≤—ã–±–æ—Ä–∫–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip(file_id, archive_path, target_dir):\n",
    "    if os.path.isdir(target_dir) and os.listdir(target_dir):\n",
    "        print(f\"Directory '{target_dir}' is not empty. Skipping download.\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(archive_path), exist_ok=True)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading archive ID {file_id} to {archive_path}...\")\n",
    "    gdown.download(id=file_id, output=archive_path, quiet=False, fuzzy=True)\n",
    "    print(\"Download complete.\")\n",
    "    \n",
    "    print(f\"Unzipping archive '{archive_path}' to '{target_dir}'...\")\n",
    "    with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(target_dir) \n",
    "    print(\"Unzipping complete.\")\n",
    "    \n",
    "    os.remove(archive_path)\n",
    "    print(f\"Archive '{archive_path}' deleted.\")\n",
    "    \n",
    "DATASET_ID = \"1bnYNmmMpb0eXr028qDzB9jHNUwJTCghl\" \n",
    "TEMP_DIR = \"data/temp\" \n",
    "ARCHIVE_PATH = os.path.join(TEMP_DIR, \"celeba_128.zip\")\n",
    "FINAL_IMAGE_DIR = \"data/celeba_hq/\" \n",
    "\n",
    "download_and_unzip(DATASET_ID, ARCHIVE_PATH, FINAL_IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ec7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, root_dir: str, transform=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = ...\n",
    "        \n",
    "    def __len__(self):\n",
    "        ...\n",
    "    def __getitem__(self, idx):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(42) # for train test split\n",
    "\n",
    "#‚ï∞( Õ°¬∞ Õú ñ Õ°¬∞ )„Å§‚îÄ‚îÄ‚òÜ*:„ÉªÔæü \n",
    "# Your code here\n",
    "\n",
    "train_dataset, val_dataset = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a89de9",
   "metadata": {},
   "source": [
    "–í–∑–≥–ª—è–Ω–µ–º –Ω–∞ –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = random.sample(range(len(train_dataset)), 20)\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    ax = axes[i]\n",
    "    img_tensor = train_dataset[idx]\n",
    "    \n",
    "    img_display = img_tensor.permute(1, 2, 0) * 0.5 + 0.5\n",
    "    ax.imshow(img_display)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3eb20a",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 2: Equalized Learning Rate\n",
    "\n",
    "–í —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö –º—ã –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Å–æ–≤ –æ–¥–∏–Ω —Ä–∞–∑ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ–±—ã –ø—Ä–∏–≤–µ—Å—Ç–∏ –¥–∏—Å–ø–µ—Ä—Å–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–π –∫ —Ä–∞–∑—É–º–Ω—ã–º –ø—Ä–µ–¥–µ–ª–∞–º. –û–¥–Ω–∞–∫–æ –≤ `GAN`-–∞—Ö, –≤–µ—Å–∞ –º–æ–≥—É—Ç —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º —Å–∏–ª—å–Ω–æ –∏–∑–º–µ–Ω—è—Ç—å—Å—è, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.\n",
    "\n",
    "–ê–≤—Ç–æ—Ä—ã ProGAN –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **Equalized Learning Rate** ‚Äî –º–µ—Ö–∞–Ω–∏–∑–º, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –º–∞—Å—à—Ç–∞–± –≤–µ—Å–æ–≤ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–±–æ—Ç—ã —Å–µ—Ç–∏.\n",
    "\n",
    "–û—Å–Ω–æ–≤–Ω—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —è–≤–ª—è–µ—Ç—Å—è $\\text{fan\\_in}$, –∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–¥–Ω—ã—Ö —Å–≤—è–∑–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–µ–π—Ä–æ–Ω–∞. –ß–µ–º –±–æ–ª—å—à–µ $\\text{fan\\_in}$, —Ç–µ–º —Å–∏–ª—å–Ω–µ–µ ¬´–≤–∑—Ä—ã–≤–∞–µ—Ç—Å—è¬ª —Å–∏–≥–Ω–∞–ª. –î–ª—è –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏ —ç—Ç–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞ –±—ã–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–∞—Å—à—Ç–∞–±–∞ $c$:\n",
    "\n",
    "$$c = \\sqrt{2 / \\text{fan\\_in}}$$\n",
    "\n",
    "–ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–∞–∫:\n",
    "\n",
    "1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ—Å–∞ –∏–∑ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è $\\mathcal{N}(0, 1)$.\n",
    "\n",
    "2. –ù–µ –º–µ–Ω—è–µ–º –≤–µ—Å–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –∞ –≤—ã—á–∏—Å–ª—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—â–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç $c$ –Ω–∞ –æ—Å–Ω–æ–≤–µ $\\text{fan\\_in}$.\n",
    "\n",
    "3. –í–æ –≤—Ä–µ–º—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –º—ã —É–º–Ω–æ–∂–∞–µ–º –Ω–∞—à–∏ –≤–µ—Å–∞ –Ω–∞ —ç—Ç–æ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç\n",
    "\n",
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ö–∞–Ω–∏–∑–º `Equalized Learning Rate` –≤ –ª–∏–Ω–µ–π–Ω–æ–º –∏ —Å–≤—ë—Ä—Ç–æ—á–Ω–æ–º —Å–ª–æ—è—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845bf92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with Equalized Learning Rate\n",
    "    Used in the Mapping Network and for Style Modulation\n",
    "    Args:\n",
    "        input_size (int): Size of input sample\n",
    "        output_size (int): Size of output sample\n",
    "        gain (float): Activation scaling factor, usually sqrt(2)\n",
    "        lrmul (float): Learning rate multiplier\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, gain=2**0.5, lrmul=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calculate He initialization constant: gain * sqrt(1 / fan_in)\n",
    "        # For Linear, fan_in = input_size\n",
    "        he_std = ... \n",
    "        \n",
    "        self.w_mul = he_std * lrmul\n",
    "        self.b_mul = lrmul\n",
    "        init_std = 1.0 / lrmul\n",
    "            \n",
    "        # Initialize weights with Standard Normal Distribution N(0, 1) scaled by init_std\n",
    "        self.weight = nn.Parameter(...) \n",
    "        self.bias = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Scale the weights and bias using self.w_mul and self.b_mul before passing to F.linear \n",
    "        scaled_weight = ...\n",
    "        scaled_bias = ...\n",
    "        return F.linear(x, scaled_weight, scaled_bias)\n",
    "    \n",
    "\n",
    "class EqualizedConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional layer with Equalized Learning Rate (EqLR)\n",
    "    Args:\n",
    "        input_channels (int): Number of input channels\n",
    "        output_channels (int): Number of output channels\n",
    "        kernel_size (int): Kernel size \n",
    "        stride (int): Stride of the convolution\n",
    "        gain (float): Activation scaling factor\n",
    "        lrmul (float): Learning rate multiplier\n",
    "        padding (int): Zero-padding\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, stride=1, gain=2**0.5, lrmul=1.0, padding=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calculate He initialization constant\n",
    "        # For Conv2d, fan_in = input_channels * kernel_width * kernel_height\n",
    "        fan_in = ...\n",
    "        he_std = ...\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.w_mul = he_std * lrmul\n",
    "        self.b_mul = lrmul\n",
    "        init_std = 1.0 / lrmul\n",
    "\n",
    "        # Initialize self.weight with N(0, 1) scaled by init_std\n",
    "        self.weight = nn.Parameter(...)\n",
    "        self.bias = nn.Parameter(torch.zeros(output_channels))\n",
    "            \n",
    "        self.stride = stride\n",
    "        self.padding = padding if padding is not None else kernel_size // 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Scale weights by self.w_mul and bias by self.b_mul\n",
    "        scaled_weight = ...\n",
    "        scaled_bias = ...\n",
    "        return F.conv2d(\n",
    "            x,\n",
    "            scaled_weight, \n",
    "            scaled_bias,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb2328e",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 3: Pixel Normalization Layer, NoiseLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf32f5",
   "metadata": {},
   "source": [
    "#### Pixel Normalization Layer\n",
    "\n",
    "–ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã `GAN` —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å —Å —Ç–µ–º, —á—Ç–æ `Batch Normalization` –Ω–µ —Ä–∞–±–æ—Ç–∞–ª–∞ –¥–æ–ª–∂–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –∏–∑-–∑–∞ —Ç–æ–≥–æ, —á—Ç–æ —Ä–∞–∑–º–µ—Ä—ã –±–∞—Ç—á–µ–π —á–∞—Å—Ç–æ –±—ã–ª–∏ —Å–ª–∏—à–∫–æ–º –º–∞–ª—ã, –∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –Ω–∏–º ‚Äî —Å–ª–∏—à–∫–æ–º —à—É–º–Ω–æ–π.\n",
    "\n",
    "–ê–≤—Ç–æ—Ä—ã `ProGAN` –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ –ø–∏–∫—Å–µ–ª—è–º** (**Pixel Normalization**).\n",
    "\n",
    "–í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Å–µ–º—É –±–∞—Ç—á—É, –º—ã –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º $L_2$-–Ω–æ—Ä–º—É –≤–µ–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–¥–æ–ª—å –∫–∞–Ω–∞–ª–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–∏–∫—Å–µ–ª—è. –¢–æ –µ—Å—Ç—å, –¥–ª—è –∫–∞–∂–¥–æ–≥–æ $(x, y)$ –ø–∏–∫—Å–µ–ª—è –º—ã –±–µ—Ä–µ–º –µ–≥–æ –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ $C$ –∫–∞–Ω–∞–ª–∞–º –∏ –ø—Ä–∏–≤–æ–¥–∏–º –µ–≥–æ –¥–ª–∏–Ω—É –∫ –µ–¥–∏–Ω–∏—Ü–µ:\n",
    "\n",
    "$$b_{x,y} = \\frac{a_{x,y}}{\\sqrt{\\frac{1}{C} \\sum_{j=0}^{C-1} (a_{x,y}^j)^2 + \\epsilon}}$$\n",
    "\n",
    "–≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –∏ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–µ–Ω –≤ —Å–∞–º–æ–º –Ω–∞—á–∞–ª–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞, –≤ `Mapping Network`, –≥–¥–µ –æ–Ω –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –≤—Ö–æ–¥–Ω–æ–º—É –≤–µ–∫—Ç–æ—Ä—É $\\mathbf{z}$.\n",
    "\n",
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥ `forward` –¥–ª—è —Å–ª–æ—è `PixelNormLayer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3203d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNormLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Pixel Normalization\n",
    "    Used in the Mapping Network to normalize the magnitude of feature vectors\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Calculate Mean Square across the channel dimension (dim=1)\n",
    "        ms = ...\n",
    "\n",
    "        # Calculate the inverse Root Mean Square (rRMS), use torch.rsqrt() for computational efficiency\n",
    "\n",
    "        rrms = ...\n",
    "        \n",
    "        # Apply rRMS to the input tensor x\n",
    "        x_norm = ...\n",
    "        \n",
    "        return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42413ce",
   "metadata": {},
   "source": [
    "#### NoiseLayer\n",
    "\n",
    "–í –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö `GAN` –≤–µ–∫—Ç–æ—Ä $\\mathbf{z}$ –æ—Ç–≤–µ—á–∞–ª —Å—Ä–∞–∑—É –∏ –∑–∞ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –∏ –∑–∞ –º–µ–ª–∫–∏–µ –¥–µ—Ç–∞–ª–∏. –ü–æ—ç—Ç–æ–º—É, –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –±—ã–ª–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è, —Å–µ—Ç—å –ø—ã—Ç–∞–ª–∞—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏—Ö –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫. –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏–ª–æ –∫ —Ç–æ–º—É, —á—Ç–æ –º–µ–ª–∫–∏–µ –¥–µ—Ç–∞–ª–∏ –≤—ã–≥–ª—è–¥–µ–ª–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏–ª–∏ –∫ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –ª–∏—Ü–∞.\n",
    "\n",
    "–í `StyleGAN` —Ä–µ—à–∏–ª–∏ —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –≤–≤–µ–¥—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π **—Å–ª–æ–π —à—É–º–∞** (**Noise Layer**) –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è –≤ —Å–µ—Ç–∏ —Å–∏–Ω—Ç–µ–∑–∞.\n",
    "\n",
    "–≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—É —Ä–∞–∑–¥–µ–ª–∏—Ç—å –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏:\n",
    "\n",
    "- **–í–µ–∫—Ç–æ—Ä —Å—Ç–∏–ª—è** $\\mathbf{w}$: –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã (–ø–æ–∑–∞, –≤–æ–∑—Ä–∞—Å—Ç)\n",
    "\n",
    "- **–°–ª–æ–π —à—É–º–∞**: –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –º–µ–ª–∫–∏–µ –¥–µ—Ç–∞–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –≤–ª–∏—è—é—Ç –Ω–∞ –æ–±—â—É—é –∫–∞—Ä—Ç–∏–Ω—É\n",
    "\n",
    "–®—É–º –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–∞–µ–º—ã—Ö –≤–µ—Å–æ–≤ `self.weight`. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–µ—Ç–∏ —Ä–µ—à–∞—Ç—å, —Å–∫–æ–ª—å–∫–æ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞.\n",
    "\n",
    "–í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ `forward` –≤ —Å–ª–æ–µ `NoiseLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b63836",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Injects per-pixel Gaussian noise, scaled by a learned per-channel weight.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(channels))\n",
    "        self.noise = None \n",
    "\n",
    "    def forward(self, x, noise=None):\n",
    "        # x shape: [B, C, H, W]\n",
    "        \n",
    "        if noise is None and self.noise is None:\n",
    "            # Create the noise tensor using torch.randn (noise must be 1-channel for broadcasting)\n",
    "            noise = ...\n",
    "\n",
    "        elif noise is None:\n",
    "            noise = self.noise\n",
    "        \n",
    "        # Scale the noise by the learned weight and add to features\n",
    "       \n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74f09ec",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 4: AdaIN \n",
    "\n",
    "–í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –ø—Ä–æ—Å—Ç–æ –ø–æ–¥–∞–≤–∞—Ç—å –≤–µ–∫—Ç–æ—Ä —Å—Ç–∏–ª—è $\\mathbf{w}$ –≤ –Ω–∞—á–∞–ª–µ, `StyleGAN` –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–≥–æ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —É—Ä–æ–≤–Ω–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –≠—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∑–∞ —Å—á–µ—Ç **–∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏** (**Adaptive Instance Normalization**, **AdaIN**).\n",
    "\n",
    "AdaIN —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —ç—Ç–∞–ø–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º —Å –ø–æ–º–æ—â—å—é –¥–≤—É—Ö –∫–ª–∞—Å—Å–æ–≤:\n",
    "\n",
    "- **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è** $\\mathbf{x}_i \\to \\mathbf{x}'$: –°–Ω–∞—á–∞–ª–∞ —Å—Ç–∏—Ä–∞–µ—Ç—Å—è —Å—Ç–∞—Ä—ã–π —Å—Ç–∏–ª—å –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø—Ä–∏—á–µ–º –∫–∞–∂–¥–∞—è –∫–∞—Ä—Ç–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ $\\mathbf{x}_i$ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ. –≠—Ç–æ—Ç —ç—Ç–∞–ø —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é `Instance Normalization` –≤–Ω—É—Ç—Ä–∏ `StyleBlock`.\n",
    "\n",
    "- **–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è** ($\\mathbf{w} \\to \\mathbf{y} \\to \\mathbf{b}$): –ó–∞—Ç–µ–º –º—ã –≤–Ω–µ–¥—Ä—è–µ–º –Ω–æ–≤—ã–π —Å—Ç–∏–ª—å. –í–µ–∫—Ç–æ—Ä $\\mathbf{w}$ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –æ–±—É—á–∞–µ–º—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –∏ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ –≤–µ–∫—Ç–æ—Ä —Å—Ç–∏–ª—è $\\mathbf{y} = (y_s, y_b)$. –≠—Ç–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è ($y_{s,i}$) –∏ —Å–¥–≤–∏–≥–∞ ($y_{b,i}$) –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ $\\mathbf{x}'$. –≠—Ç–æ—Ç —ç—Ç–∞–ø —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –≤ –∫–ª–∞—Å—Å–µ `StyleModulation`.\n",
    "\n",
    "–§–æ—Ä–º—É–ª–∞, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∞—è —ç—Ç—É –æ–ø–µ—Ä–∞—Ü–∏—é:\n",
    "\n",
    "$$\\text{AdaIN}(\\mathbf{x}_i, \\mathbf{y}) = y_{s,i} \\frac{\\mathbf{x}_i - \\mu(\\mathbf{x}_i)}{\\sigma(\\mathbf{x}_i)} + y_{b,i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b967e3",
   "metadata": {},
   "source": [
    "#### StyleModulation\n",
    "\n",
    "–≠—Ç–æ—Ç –∫–ª–∞—Å—Å –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é. –û–Ω –±–µ—Ä–µ—Ç –≤–µ–∫—Ç–æ—Ä $\\mathbf{w}$, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –µ–≥–æ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ç–∏–ª—è $y_{s,i}$ –∏ $y_{b,i}$ –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∏—Ö –∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –∫–∞—Ä—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "\n",
    "–í–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –∫–ª–∞—Å—Å–µ `StyleModulation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64225169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleModulation(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies the scale (ys) and bias (yb) parameters derived from w.\n",
    "    This performs the Adaptive part of the AdaIN operation.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_size, channels): \n",
    "        super().__init__()\n",
    "        self.lin = EqualizedLinear(latent_size, channels * 2, gain=1.0, lrmul=1.0)\n",
    "\n",
    "    def forward(self, x, latent):\n",
    "        style = self.lin(latent)\n",
    "        \n",
    "        # Reshape style into [B, 2 (scale/bias), C, 1, 1] for broadcasting\n",
    "        shape = [-1, 2, x.size(1)] + (x.dim() - 2) * [1]\n",
    "        style = style.view(shape)\n",
    "        \n",
    "        # get ys and yb (ys corresponds to style[:, 0], yb to style[:, 1])\n",
    "        ys = ...\n",
    "        yb = ...\n",
    "        \n",
    "        #Apply the final AdaIN formula\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5474ddd9",
   "metadata": {},
   "source": [
    "#### StyleBlock\n",
    "\n",
    "–≠—Ç–æ—Ç –∫–ª–∞—Å—Å —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –æ–ø–µ—Ä–∞—Ü–∏–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–µ. –ï–≥–æ –æ—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ‚Äî –æ–±–µ—Å–ø–µ—á–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: \n",
    "\n",
    "`Noise` $\\to$ `Activation` $\\to$ `InstanceNorm` $\\to$ `StyleAdjustment`. \n",
    "\n",
    "`Instance Norm` —Å—Ç–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ä—ã–π —Å—Ç–∏–ª—å, –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—è –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫ –Ω–æ–≤–æ–º—É —Å—Ç–∏–ª—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The full synthesis pipeline: Noise -> Activation -> Norm -> StyleAdjustment.\n",
    "    Args:\n",
    "        channels (int): Number of feature channels in the input map\n",
    "        dlatent_size (int): Dimensionality of the intermediate latent space\n",
    "        activation_layer (nn.Module): The activation function to use\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, dlatent_size, activation_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        \n",
    "        layers.append(('noise', NoiseLayer(channels)))\n",
    "        layers.append(('activation', activation_layer))\n",
    "        layers.append(('instance_norm', nn.InstanceNorm2d(channels)))\n",
    "\n",
    "        self.main_ops = nn.Sequential(OrderedDict(layers))\n",
    "        self.style_mod = StyleModulation(latent_size=dlatent_size, channels=channels) \n",
    "\n",
    "    def forward(self, x, dlatents_in_slice=None):\n",
    "        \"\"\"\n",
    "        Processes the feature map and applies style adjustment.\n",
    "        :param x: The input feature map\n",
    "        :param dlatents_in_slice: A slice of the W vector containing the style parameters for this layer \n",
    "        \"\"\"\n",
    "        # Get x_norm using main_ops\n",
    "        x_norm = ...\n",
    "        \n",
    "        # Apply Style Modulation\n",
    "        if self.style_mod is not None:\n",
    "            return ...\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc785f78",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 5: Minibatch Standard Deviation\n",
    "\n",
    "–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö `GAN` —è–≤–ª—è–µ—Ç—Å—è **–∫–æ–ª–ª–∞–ø—Å –º–æ–¥** (**Mode Collapse**) ‚Äî –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –Ω–∞—Ö–æ–¥–∏—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —É—Å–ø–µ—à–Ω–æ –æ–±–º–∞–Ω—ã–≤–∞—é—Ç –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä, –∞ –∑–∞—Ç–µ–º —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–µ –æ–±—ä–µ–∫—Ç—ã —Ç–æ–ª—å–∫–æ –∏–∑ —ç—Ç–æ–π –º–æ–¥—ã.\n",
    "\n",
    "–û–±—ã—á–Ω—ã–π –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –¥—Ä—É–≥–∏—Ö –≤ –±–∞—Ç—á–µ, –ø–æ—ç—Ç–æ–º—É –æ–Ω –Ω–µ –º–æ–∂–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–µ–Ω –≤–µ—Å—å –±–∞—Ç—á —Ñ–µ–π–∫–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. \n",
    "\n",
    "–ò–¥–µ—è **Minibatch Standard Deviation** (**MBSD**) –±—ã–ª–∞ –≤–ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ 2016 –≥–æ–¥—É [Salimans et al.](https://arxiv.org/pdf/1606.03498) –∏ –±—ã–ª–∞ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ `ProGAN`. **MBSD** —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–ª–ª–∞–ø—Å–∞ –º–æ–¥, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä—É –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –º–∏–Ω–∏-–±–∞—Ç—á–∞.\n",
    "\n",
    "**MBSD** —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –¥–µ—Ç–µ–∫—Ç–æ—Ä –æ–¥–Ω–æ–æ–±—Ä–∞–∑–∏—è, –∫–æ—Ç–æ—Ä—ã–π –≥–æ–≤–æ—Ä–∏—Ç –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä—É, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –±–∞—Ç—á–µ:\n",
    "\n",
    "- –°–ª–æ–π –¥–µ–ª–∏—Ç –º–∏–Ω–∏-–±–∞—Ç—á –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏–µ –≥—Ä—É–ø–ø—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "\n",
    "- –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ $\\mathbf{x}$ –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ $\\boldsymbol{\\sigma}$ –ø–æ –æ—Å–∏ –±–∞—Ç—á–∞. –≠—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –ø—Ä–∏–∑–Ω–∞–∫ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö\n",
    "\n",
    "- –í—Å–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è $\\boldsymbol{\\sigma}$ —É—Å—Ä–µ–¥–Ω—è—é—Ç—Å—è –ø–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º –∏ –∫–∞–Ω–∞–ª–∞–º\n",
    "\n",
    "- –≠—Ç–æ—Ç –≤–µ–∫—Ç–æ—Ä –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫–∞–∫ –Ω–æ–≤—ã–π –∫–∞–Ω–∞–ª –∫ –≤—ã—Ö–æ–¥–Ω–æ–π –∫–∞—Ä—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞\n",
    "\n",
    "–í–∞–º –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —à–∞–≥–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è $\\boldsymbol{\\sigma}$, –æ–ø–∏—Å–∞–Ω–Ω—ã–µ –≤—ã—à–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9db235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchStddev(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates the standard deviation of features across the minibatch group \n",
    "    and appends it as a new feature map to discourage mode collapse.\n",
    "    Args:\n",
    "        group_size (int): The number of samples to group together for statistics, must be a divisor of the batch size \n",
    "        num_new_features (int): Number of new feature maps to append\n",
    "    \"\"\"\n",
    "    def __init__(self, group_size=4, num_new_features=1):\n",
    "        super().__init__()\n",
    "        self.group_size = group_size\n",
    "        self.num_new_features = num_new_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        group_size = min(self.group_size, b)\n",
    "        \n",
    "        # Reshape to [G, M, F, C', H, W] where G is group_size, M is Minibatches, F is num_new_features, C' is Internal channels\n",
    "        y = x.view(group_size, -1, self.num_new_features, c // self.num_new_features, h, w)\n",
    "        \n",
    "        # Calculate the standard deviation sqrt(Mean((y - mean(y))^2) + epsilon)\n",
    "        y_std = ...\n",
    "\n",
    "        # Average Std Dev over all non-group dimensions (C', H, W)\n",
    "        y_avg = ...\n",
    "        \n",
    "        # Replicate y_avg over the group dimension and expand to [B, F, H, W]\n",
    "        y_final = ...\n",
    "        \n",
    "        #  Concatenate x and y along the channel dimension\n",
    "        z = ...\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375503b",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 6: Truncation Trick \n",
    "\n",
    "–û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ª—É—á—à–µ –≤—Å–µ–≥–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤—ã—Å–æ–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, —Ç–æ –µ—Å—Ç—å, —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã—Ö –∏–ª–∏ \"—Ç–∏–ø–∏—á–Ω—ã—Ö\" –ª–∏—Ü. –û–±–ª–∞—Å—Ç–∏ —Å –Ω–∏–∑–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –Ω–µ—Ç–∏–ø–∏—á–Ω—ã–µ, —Å–∏–ª—å–Ω–æ –∏—Å–∫–∞–∂–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã.\n",
    "\n",
    "**–¢—Ä—é–∫ —É—Å–µ—á–µ–Ω–∏—è** (**Truncation Trick**) ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ï–≥–æ —Ü–µ–ª—å ‚Äî –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –ª—é–±–æ–π —Å–ª—É—á–∞–π–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —Å—Ç–∏–ª—è $\\mathbf{w}$ –∏–∑ –æ–±–ª–∞—Å—Ç–µ–π —Å –Ω–∏–∑–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é –æ–±—Ä–∞—Ç–Ω–æ –∫ —Å—Ä–µ–¥–Ω–µ–º—É –≤–µ–∫—Ç–æ—Ä—É —Å—Ç–∏–ª—è $\\bar{\\mathbf{w}}$. \n",
    "\n",
    "–° –æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, —ç—Ç–æ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏ —É–º–µ–Ω—å—à–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤, –∞ —Å –¥—Ä—É–≥–æ–π ‚Äî —Å–Ω–∏–∂–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤.\n",
    "\n",
    "–¢—Ä—é–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n",
    "\n",
    "- –í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –º—ã —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ $\\bar{\\mathbf{w}}$ –≤—Å–µ—Ö —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å—Ç–∏–ª—è $\\mathbf{w}$.\n",
    "\n",
    "- –ü—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä $\\mathbf{w}$ —Å–º–µ—à–∏–≤–∞–µ—Ç—Å—è —Å $\\bar{\\mathbf{w}}$ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ $\\psi$ (`threshold`):\n",
    "\n",
    "$$\\mathbf{w}' = \\bar{\\mathbf{w}} + \\psi \\cdot (\\mathbf{w} - \\bar{\\mathbf{w}})$$\n",
    "\n",
    "–í `StyleGAN` —ç—Ç–æ—Ç —Ç—Ä—é–∫ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫ –ø–µ—Ä–≤—ã–º `max_layer` –≤–µ–∫—Ç–æ—Ä–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—é—Ç –≥—Ä—É–±—ã–µ, –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã.\n",
    "\n",
    "–í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ª–æ–≥–∏–∫—É **Truncation Trick**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Truncation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Truncation Trick in W-space to bias random samples \n",
    "    towards the mean, improving fidelity at the cost of diversity.\n",
    "    \"\"\"\n",
    "    def __init__(self, avg_latent, max_layer=8, threshold=0.7, beta=0.995):\n",
    "        super().__init__()\n",
    "        self.max_layer = max_layer\n",
    "        self.threshold = threshold\n",
    "        self.beta = beta\n",
    "        self.register_buffer('avg_latent', avg_latent)\n",
    "\n",
    "    def update(self, last_avg):\n",
    "        \"\"\"\n",
    "        Calculates the new exponential moving average (EMA) of the W vector.\n",
    "        :param last_avg: The mean W vector of the current training batch.\n",
    "        \"\"\"\n",
    "        # Implement the EMA : new_avg = beta * old_avg + (1 - beta) * current_avg\n",
    "        self.avg_latent.copy_(...) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply Truncation formula, use torch.lerp\n",
    "        interp = ...\n",
    "        \n",
    "        # Create the masking tensor (only for layers < max_layer)\n",
    "        do_trunc = ...\n",
    "        \n",
    "        # Apply the mask\n",
    "\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cdfc10",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 7: Constant Input Block\n",
    "\n",
    "–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π, –≥–¥–µ —Å–∫—Ä—ã—Ç—ã–π –≤–µ–∫—Ç–æ—Ä $\\mathbf{z}$ –Ω–∞–ø—Ä—è–º—É—é –ø–æ–¥–∞–≤–∞–ª—Å—è –≤ –ø–µ—Ä–≤—ã–π —Å–ª–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞, `StyleGAN` –Ω–∞—á–∏–Ω–∞–µ—Ç —Å–∏–Ω—Ç–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ —Å —à—É–º–∞, –∞ —Å –æ–±—É—á–∞–µ–º–æ–π –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã. –ó–∞ —ç—Ç–æ –æ—Ç–≤–µ—á–∞–µ—Ç –∫–ª–∞—Å—Å `ConstantInputBlock`.\n",
    "\n",
    "–û–±—É—á–∞–µ–º–∞—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ `self.const` –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–æ–ª—å —Ö–æ–ª—Å—Ç–∞ ‚Äî –æ–±—É—á–∞–µ–º–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ —Ä–∞–∑–º–µ—Ä–∞ $4 \\times 4 \\times 512$, –∫–æ—Ç–æ—Ä—ã–π –¥–∞—ë—Ç –Ω–∞—á–∞–ª—å–Ω—É—é –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ü–æ—Å–∫–æ–ª—å–∫—É —ç—Ç–æ—Ç —Ö–æ–ª—Å—Ç —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—É –Ω–µ –Ω—É–∂–Ω–æ —Ç—Ä–∞—Ç–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –Ω–∞ –µ—ë —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ –≤–µ–∫—Ç–æ—Ä–∞ $\\mathbf{z}$.\n",
    "\n",
    "–ó–∞–¥–∞—á–∞ `ConstantInputBlock` —Å–≤–æ–¥–∏—Ç—Å—è –∫ —Ç–æ–º—É, —á—Ç–æ–±—ã –≤–∑—è—Ç—å —ç—Ç–æ—Ç –æ–±—É—á–∞–µ–º—ã–π —Ç–µ–Ω–∑–æ—Ä –∏ –Ω–∞—á–∞—Ç—å –µ–≥–æ —Å—Ç–∏–ª–∏–∑–∞—Ü–∏—é:\n",
    "\n",
    "- –°–Ω–∞—á–∞–ª–∞ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ç–µ–Ω–∑–æ—Ä—É –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ø–µ—Ä–≤—ã–π –≤–µ–∫—Ç–æ—Ä —Å—Ç–∏–ª—è $\\mathbf{w}_0$, —á—Ç–æ –∑–∞–¥–∞–µ—Ç –≥—Ä—É–±—ã–µ, –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "\n",
    "- –ó–∞—Ç–µ–º —Å–ª–µ–¥—É–µ—Ç —Å–≤–µ—Ä—Ç–∫–∞, –∞ –ø–æ—Å–ª–µ –Ω–µ–µ ‚Äî –≤—Ç–æ—Ä–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ —Å—Ç–∏–ª—è $\\mathbf{w}_1$\n",
    "\n",
    "–í–∞–º –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ª–æ–≥–∏–∫—É `ConstantInputBlock`, –∑–∞–ø–æ–ª–Ω–∏–≤ –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –∫–æ–¥–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantInputBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The initial 4x4 block of the StyleGAN Generator. \n",
    "    It replaces the direct Z input with a trainable constant\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_channels, \n",
    "                 dlatent_size, \n",
    "                 activation_layer):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize self.const with a tensor of ones\n",
    "        self.const = nn.Parameter(...) \n",
    "        \n",
    "        # We also need a bias parameter for the constant input\n",
    "        self.bias = nn.Parameter(...)\n",
    "        \n",
    "        self.style_block1 = StyleBlock(\n",
    "            channels=num_channels, \n",
    "            dlatent_size=dlatent_size, \n",
    "            activation_layer=activation_layer\n",
    "        )\n",
    "        \n",
    "        self.conv = EqualizedConv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.style_block2 = StyleBlock(\n",
    "            channels=num_channels, \n",
    "            dlatent_size=dlatent_size,  \n",
    "            activation_layer=activation_layer\n",
    "        )\n",
    "\n",
    "    def forward(self, w_slice):\n",
    "        # w_slice contains the first two W vectors: [B, 2, D]\n",
    "        batch_size = w_slice.size(0)\n",
    "\n",
    "        # Expand the constant to match the batch size [1, C, 4, 4] -> [B, C, 4, 4]\n",
    "        x = ...\n",
    "        \n",
    "        # Add the learned bias\n",
    "        x = ...\n",
    "\n",
    "        # Apply the first style w_0\n",
    "        x = ...\n",
    "        \n",
    "        # Pass through convolution\n",
    "        x = ...\n",
    "        \n",
    "        # Apply the second style w1\n",
    "        x = ...\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e686a0",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 8: Synthesis Block\n",
    "\n",
    "`SynthesisBlock` ‚Äî —ç—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π –±–ª–æ–∫, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏ –≤—ã–¥–∞–µ—Ç –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–¥–≤–æ–µ –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.\n",
    "\n",
    "–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç `ConstantInputBlock`, –∫–æ—Ç–æ—Ä—ã–π —Ç–æ–ª—å–∫–æ —Å–æ–∑–¥–∞–µ—Ç –∑–∞–≥–æ—Ç–æ–≤–∫—É $4 \\times 4$, —ç—Ç–æ—Ç –±–ª–æ–∫ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º—Å—è —Ü–∏–∫–ª–æ–º —Å–∏–Ω—Ç–µ–∑–∞, —Å–æ—Å—Ç–æ—è—â–∏–º –∏–∑ 3 —ç—Ç–∞–ø–æ–≤:\n",
    "\n",
    "1. **Upsampling**\n",
    "\n",
    "2. **First Convolution and Style Adjustment**\n",
    "\n",
    "3. **Second Convolution and Final Style Adjustment**\n",
    "\n",
    "–ó–∞–º–µ—Ç—å—Ç–µ, —á—Ç–æ –±–ª–æ–∫ –ø–æ—Ç—Ä–µ–±–ª—è–µ—Ç –¥–≤–∞ –≤–µ–∫—Ç–æ—Ä–∞ —Å—Ç–∏–ª—è –Ω–∞ –æ–¥–Ω–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ. –≠—Ç–æ —Å–¥–µ–ª–∞–Ω–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã —É–≤–µ–ª–∏—á–∏—Ç—å –µ–º–∫–æ—Å—Ç—å –∏ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å —Å–µ—Ç–∏. –ü–µ—Ä–≤—ã–π –≤–µ–∫—Ç–æ—Ä —Å—Ç–∏–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –ø–æ–≤—ã—à–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è, –∞ –≤—Ç–æ—Ä–æ–π ‚Äî —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏. \n",
    "\n",
    "–ë–ª–∞–≥–æ–¥–∞—Ä—è —Ç–∞–∫–æ–º—É **Style Mixing** –º—ã –º–æ–∂–µ–º —Å–º–µ—à–∏–≤–∞—Ç—å –∞—Ç—Ä–∏–±—É—Ç—ã, –≤–∑—è—Ç—ã–µ —Å —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2eeab4",
   "metadata": {},
   "source": [
    "#### Upsample, DownSample\n",
    "\n",
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ `StyleGAN` –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –Ω–∞ –∏–¥–µ–µ **Progressive Growing**, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–π **–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä** –∏ **–¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä** —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –ê –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –º–µ–∂–¥—É —ç—Ç–∏–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏–∑–º–µ–Ω–µ–Ω—è—Ç—å —Ä–∞–∑–º–µ—Ä—ã –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n",
    "\n",
    "- **Upsampling**: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "\n",
    "- **Downsampling**: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–æ–º –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "\n",
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∏ —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ `PyTorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthesisBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The main repeating block of the Generator pipeline. \n",
    "    Doubles the resolution and consumes two W vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dlatent_size, gain, activation_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        self.conv1 = EqualizedConv2d(in_channels, \n",
    "                                     out_channels, \n",
    "                                     kernel_size=3, \n",
    "                                     padding=1, \n",
    "                                     gain=gain\n",
    "                                     )\n",
    "        \n",
    "        self.style_block1 = StyleBlock(channels=out_channels, \n",
    "                                       dlatent_size=dlatent_size,  \n",
    "                                       activation_layer=activation_layer\n",
    "                                       )\n",
    "        \n",
    "        self.conv2 = EqualizedConv2d(out_channels, \n",
    "                                     out_channels, \n",
    "                                     kernel_size=3, \n",
    "                                     padding=1, \n",
    "                                     gain=gain\n",
    "                                     )\n",
    "        self.style_block2 = StyleBlock(channels=out_channels, \n",
    "                                       dlatent_size=dlatent_size, \n",
    "                                       activation_layer=activation_layer\n",
    "                                       )\n",
    "\n",
    "    def forward(self, x, w_slice):\n",
    "        \n",
    "        # Apply upsample to the input x\n",
    "        x = ... \n",
    "        # Apply conv1 to the result\n",
    "        x = ...\n",
    "        # Apply self.style_block1, using the first w vector slice\n",
    "        x = ...\n",
    "        # Apply self.conv2\n",
    "        x = ...\n",
    "        # Apply self.style_block2, using the second w vector slice\n",
    "        x = ...\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f7209",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 9: Discriminator Output Block\n",
    "\n",
    "`DiscriminatorOutputBlock` —è–≤–ª—è–µ—Ç—Å—è —Ñ–∏–Ω–∞–ª—å–Ω—ã–º —ç—Ç–∞–ø–æ–º —Ä–∞–±–æ—Ç—ã –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞,  –≤—ã–ø–æ–ª–Ω—è—è —Ç—Ä–∏  –¥–µ–π—Å—Ç–≤–∏—è:\n",
    "\n",
    "1. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ —Å–ª–æ–π `MinibatchStddev` –∏ –ø–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–∏ –≤—Å–µ–≥–æ –±–∞—Ç—á–∞\n",
    "\n",
    "2. –í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–≤–µ—Ä—Ç–∫—É $3 \\times 3$, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ \n",
    "\n",
    "3. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–≤–µ—Ä—Ç–∫–∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –¥–≤–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ—è, –∫–æ—Ç–æ—Ä—ã–µ –∏ –≤—ã–¥–∞—é—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "\n",
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥ `forward` –≤ –∫–ª–∞—Å—Å–µ `DiscriminatorOutputBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31881435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorOutputBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Final block of the Discriminator. It applies MBSD, the last feature \n",
    "    extraction, and converts the 4x4 feature map into a single realism score.\n",
    "\n",
    "    Args:\n",
    "            mbstd_group_size (int): Number of samples per group for Minibatch Standard Deviation \n",
    "            mbstd_num_features (int): Number of new feature maps to append from MBSD\n",
    "            in_channels (int): Number of input channels to this block\n",
    "            intermediate_channels (int): Number of channels in the first fully connected layer\n",
    "            gain (float): He initialization gain for most layers\n",
    "            activation_layer (nn.Module): The activation function\n",
    "            resolution (int): The spatial resolution of the input feature map\n",
    "            in_channels2 (int, optional): Output channels of the last convolution, defaults to in_channels\n",
    "            output_features (int): Final output size \n",
    "            last_gain (float): Gain for the final output linear layer\n",
    "    \"\"\"\n",
    "    def __init__(self, mbstd_group_size, \n",
    "                 mbstd_num_features, \n",
    "                 in_channels, \n",
    "                 intermediate_channels, \n",
    "                 gain, \n",
    "                 activation_layer, \n",
    "                 resolution=4, \n",
    "                 in_channels2=None, \n",
    "                 output_features=1, \n",
    "                 last_gain=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if in_channels2 is None:\n",
    "            in_channels2 = in_channels\n",
    "            \n",
    "        if mbstd_group_size > 1:\n",
    "            self.mbstd = MinibatchStddev(mbstd_group_size, mbstd_num_features)\n",
    "            final_in_channels = in_channels + mbstd_num_features\n",
    "        else:\n",
    "            self.mbstd = None\n",
    "            final_in_channels = in_channels\n",
    "\n",
    "        self.conv = EqualizedConv2d(final_in_channels, in_channels2, kernel_size=3, padding=1, gain=gain)\n",
    "        self.act1 = activation_layer\n",
    "        self.fc1 = EqualizedLinear(in_channels2 * resolution * resolution, intermediate_channels, gain=gain)\n",
    "        self.act2 = activation_layer\n",
    "        self.fc2 = EqualizedLinear(intermediate_channels, output_features, gain=last_gain)\n",
    "\n",
    "\n",
    "    def forward(self, x):    \n",
    "\n",
    "        if self.mbstd is not None:\n",
    "            # Apply self.mbstd to the input x\n",
    "            x = ...\n",
    "            \n",
    "        # Apply activation to the result of the convolution\n",
    "        x = self.act1(self.conv(x))\n",
    "        \n",
    "        # Flatten the tensor using torch.flatten()\n",
    "        x_flat = ...\n",
    "        \n",
    "        # Apply the first linear layer, then the second activation\n",
    "        x = ...\n",
    "        \n",
    "        # Return the result of the last linear layer\n",
    "        return ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nick_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
