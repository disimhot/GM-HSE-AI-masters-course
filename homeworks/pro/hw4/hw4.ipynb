{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd279162",
   "metadata": {},
   "source": [
    "## Homework 4: StyleGAN üíáüèª‚Äç‚ôÇÔ∏è\n",
    "\n",
    "–° –º–æ–º–µ–Ω—Ç–∞ —Å–≤–æ–µ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è –≤ 2014 –≥–æ–¥—É –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤ GAN-–∞—Ö —Ä–∞–±–æ—Ç–∞–ª –∫–∞–∫ —á–µ—Ä–Ω—ã–π —è—â–∏–∫. –í —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö —Å–∫—Ä—ã—Ç—ã–π –≤–µ–∫—Ç–æ—Ä $\\mathbf{z}$ –ø–æ–¥–∞–≤–∞–ª—Å—è –≤ –ø–µ—Ä–≤—ã–π —Å–ª–æ–π —Å–µ—Ç–∏, –∏, –ø—Ä–æ—Ö–æ–¥—è —á–µ—Ä–µ–∑ —Ü–µ–ø–æ—á–∫—É —Å–≤–µ—Ä—Ç–æ–∫, –ø—Ä–µ–≤—Ä–∞—â–∞–ª—Å—è –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –•–æ—Ç—è –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä–æ—Å–ª–æ, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å —Å –ø—Ä–æ–±–ª–µ–º–æ–π —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º–æ–π **–∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç–∏** (**entanglement**) —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞. –¢–æ –µ—Å—Ç—å –ø–æ–ø—ã—Ç–∫–∞ –∏–∑–º–µ–Ω–∏—Ç—å –æ–¥–Ω—É —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –ª–∏—Ü–∞ –Ω–µ–∏–∑–±–µ–∂–Ω–æ –≤–ª–µ–∫–ª–∞ –∑–∞ —Å–æ–±–æ–π –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤.\n",
    "\n",
    "–ü—Ä–æ—Ä—ã–≤–æ–º, –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–∏–º –ø–æ—è–≤–ª–µ–Ω–∏—é `StyleGAN`, —Å—Ç–∞–ª–∞ –∏–¥–µ—è **–ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞** (**Progressive Growing**), –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∫–æ–º–∞–Ω–¥–æ–π `NVIDIA` –≤ –º–æ–¥–µ–ª–∏ [ProGAN](https://arxiv.org/pdf/1710.10196). –ò–¥–µ—è –∑–∞–∫–ª—é—á–∞–ª–∞—Å—å –≤ —Ç–æ–º, —á—Ç–æ–±—ã –Ω–∞—á–∏–Ω–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è—è –Ω–æ–≤—ã–µ —Å–ª–æ–∏ –∏ –ø–æ–≤—ã—à–∞—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –∏ –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è $1024\\times 1024$. –û–¥–Ω–∞–∫–æ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –≤ `ProGAN` –æ—Å—Ç–∞–≤–∞–ª–∞—Å—å –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç–∏ —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞.\n",
    "\n",
    "–ê–≤—Ç–æ—Ä—ã [StyleGAN](https://arxiv.org/pdf/1812.04948) –æ—Å—Ç–∞–≤–∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—É—é –∏–¥–µ—é `ProGAN`, –Ω–æ —Ä–∞–¥–∏–∫–∞–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏–ª–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞. –í–¥–æ—Ö–Ω–æ–≤–∏–≤—à–∏—Å—å –º–µ—Ç–æ–¥–∞–º–∏ **–ø–µ—Ä–µ–Ω–æ—Å–∞ —Å—Ç–∏–ª—è** (**Style Transfer**), –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç –ø–æ–¥–∞—á–∏ —Å–∫—Ä—ã—Ç–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ $\\mathbf{z}$ –≤ –ø–µ—Ä–≤—ã–π —Å–ª–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–∞—á–∏–Ω–∞—Ç—å —Å –æ–±—É—á–∞–µ–º–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞,–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å —á–µ—Ä–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—É—é **—Å–µ—Ç—å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è** (**Mapping Network**), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –ª–∞—Ç–µ–Ω—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –∏–∑ –∏—Å–∫—Ä–∏–≤–ª–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ $\\mathcal{Z}$ –≤ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ, —Ä–∞—Å–ø—É—Ç–∞–Ω–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ $\\mathcal{W}$.\n",
    "\n",
    "### –ó–∞–¥–∞–Ω–∏–µ\n",
    "\n",
    "–í–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å `StyleGAN` –∏ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ `CelebA HQ` –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª–∏—Ü.\n",
    "\n",
    "–ó–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–æ–º–∞—à–Ω–µ–≥–æ –∑–∞–¥–∞–Ω–∏—è –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –¥–æ **11 –±–∞–ª–ª–æ–≤**. –î–ª—è —á–∞—Å—Ç–∏ –∑–∞–¥–∞–Ω–∏–π –º—ã –Ω–∞–ø–∏—Å–∞–ª–∏ –¥–ª—è –≤–∞—Å —Å–∫–µ–ª–µ—Ç. –ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤ –Ω–∏—Ö –ø—Ä–æ–ø—É—Å–∫–∏, –≤—ã–¥–µ–ª–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é `...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4744da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import zipfile\n",
    "\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878275a5",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 1: Dataset (0.5 –±–∞–ª–ª–∞)\n",
    "\n",
    "–î–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç `CelebA HQ`, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç $30$ —Ç—ã—Å. –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ª–∏—Ü –∑–Ω–∞–º–µ–Ω–∏—Ç–æ—Å—Ç–µ–π –≤ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ $128\\times 128$ –ø–∏–∫—Å–µ–ª–µ–π.\n",
    "\n",
    "**–í–∞—à–∞ –∑–∞–¥–∞—á–∞**:\n",
    "\n",
    "- –°–∫–∞—á–∞—Ç—å –∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å –∞—Ä—Ö–∏–≤ —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º\n",
    "\n",
    "- –°–æ–∑–¥–∞—Ç—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: \n",
    "    - –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —Ç–µ–Ω–∑–æ—Ä (`ToTensor`).\n",
    "    - —Å–¥–µ–ª–∞—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é (`Normalize`)\n",
    "\n",
    "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏ —Å–æ–∑–¥–∞—Ç—å –∫–ª–∞—Å—Å `CelebADataset`.\n",
    "\n",
    "- –°–æ–∑–¥–∞—Ç—å `train_Loader` –¥–ª—è –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip(file_id, archive_path, target_dir):\n",
    "    if os.path.isdir(target_dir) and os.listdir(target_dir):\n",
    "        print(f\"Directory '{target_dir}' is not empty. Skipping download.\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(archive_path), exist_ok=True)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading archive ID {file_id} to {archive_path}...\")\n",
    "    gdown.download(id=file_id, output=archive_path, quiet=False, fuzzy=True)\n",
    "    print(\"Download complete.\")\n",
    "    \n",
    "    print(f\"Unzipping archive '{archive_path}' to '{target_dir}'...\")\n",
    "    with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(target_dir) \n",
    "    print(\"Unzipping complete.\")\n",
    "    \n",
    "    os.remove(archive_path)\n",
    "    print(f\"Archive '{archive_path}' deleted.\")\n",
    "    \n",
    "DATASET_ID = \"1bnYNmmMpb0eXr028qDzB9jHNUwJTCghl\" \n",
    "TEMP_DIR = \"data/temp\" \n",
    "ARCHIVE_PATH = os.path.join(TEMP_DIR, \"celeba_128.zip\")\n",
    "FINAL_IMAGE_DIR = \"data/celeba_hq/\" \n",
    "\n",
    "download_and_unzip(DATASET_ID, ARCHIVE_PATH, FINAL_IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ec7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, root_dir: str, transform=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = ...\n",
    "        \n",
    "    def __len__(self):\n",
    "        ...\n",
    "    def __getitem__(self, idx):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#‚ï∞( Õ°¬∞ Õú ñ Õ°¬∞ )„Å§‚îÄ‚îÄ‚òÜ*:„ÉªÔæü \n",
    "# Your code here\n",
    "\n",
    "train_dataset = ...\n",
    "train_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a89de9",
   "metadata": {},
   "source": [
    "–í–∑–≥–ª—è–Ω–µ–º –Ω–∞ –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = random.sample(range(len(train_dataset)), 20)\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    ax = axes[i]\n",
    "    img_tensor = train_dataset[idx]\n",
    "    \n",
    "    img_display = img_tensor.permute(1, 2, 0) * 0.5 + 0.5\n",
    "    ax.imshow(img_display)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3eb20a",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 2: Equalized Learning Rate (0.5 –±–∞–ª–ª–∞)\n",
    "\n",
    "–í —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö –º—ã –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Å–æ–≤ –æ–¥–∏–Ω —Ä–∞–∑ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ–±—ã –ø—Ä–∏–≤–µ—Å—Ç–∏ –¥–∏—Å–ø–µ—Ä—Å–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–π –∫ —Ä–∞–∑—É–º–Ω—ã–º –ø—Ä–µ–¥–µ–ª–∞–º. –û–¥–Ω–∞–∫–æ –≤ `GAN`-–∞—Ö, –≤–µ—Å–∞ –º–æ–≥—É—Ç —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º —Å–∏–ª—å–Ω–æ –∏–∑–º–µ–Ω—è—Ç—å—Å—è, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.\n",
    "\n",
    "–ê–≤—Ç–æ—Ä—ã ProGAN –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **Equalized Learning Rate** ‚Äî –º–µ—Ö–∞–Ω–∏–∑–º, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –º–∞—Å—à—Ç–∞–± –≤–µ—Å–æ–≤ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–±–æ—Ç—ã —Å–µ—Ç–∏.\n",
    "\n",
    "–û—Å–Ω–æ–≤–Ω—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —è–≤–ª—è–µ—Ç—Å—è $\\text{fan\\_in}$, –∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–¥–Ω—ã—Ö —Å–≤—è–∑–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–µ–π—Ä–æ–Ω–∞. –ß–µ–º –±–æ–ª—å—à–µ $\\text{fan\\_in}$, —Ç–µ–º —Å–∏–ª—å–Ω–µ–µ ¬´–≤–∑—Ä—ã–≤–∞–µ—Ç—Å—è¬ª —Å–∏–≥–Ω–∞–ª. –î–ª—è –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏ —ç—Ç–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞ –±—ã–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–∞—Å—à—Ç–∞–±–∞ $c$:\n",
    "\n",
    "$$c = \\sqrt{2 / \\text{fan\\_in}}$$\n",
    "\n",
    "–ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–∞–∫:\n",
    "\n",
    "1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ—Å–∞ –∏–∑ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è $\\mathcal{N}(0, 1)$.\n",
    "\n",
    "2. –ù–µ –º–µ–Ω—è–µ–º –≤–µ—Å–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –∞ –≤—ã—á–∏—Å–ª—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—â–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç $c$ –Ω–∞ –æ—Å–Ω–æ–≤–µ $\\text{fan\\_in}$.\n",
    "\n",
    "3. –í–æ –≤—Ä–µ–º—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –º—ã —É–º–Ω–æ–∂–∞–µ–º –Ω–∞—à–∏ –≤–µ—Å–∞ –Ω–∞ —ç—Ç–æ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç\n",
    "\n",
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ö–∞–Ω–∏–∑–º `Equalized Learning Rate` –≤ –ª–∏–Ω–µ–π–Ω–æ–º –∏ —Å–≤—ë—Ä—Ç–æ—á–Ω–æ–º —Å–ª–æ—è—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845bf92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with Equalized Learning Rate\n",
    "    Used in the Mapping Network and for Style Modulation\n",
    "    Args:\n",
    "        input_size: Size of input sample\n",
    "        output_size: Size of output sample\n",
    "        gain: Activation scaling factor, usually sqrt(2)\n",
    "        lrmul: Learning rate multiplier\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, gain=2**0.5, lrmul=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calculate He initialization constant: gain * sqrt(1 / fan_in)\n",
    "        # For Linear, fan_in = input_size\n",
    "        he_std = ... \n",
    "        \n",
    "        self.w_mul = he_std * lrmul\n",
    "        self.b_mul = lrmul\n",
    "        init_std = 1.0 / lrmul\n",
    "            \n",
    "        # Initialize weights with Standard Normal Distribution N(0, 1) scaled by init_std\n",
    "        self.weight = nn.Parameter(...) \n",
    "        self.bias = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Scale the weights and bias using self.w_mul and self.b_mul before passing to F.linear \n",
    "        scaled_weight = ...\n",
    "        scaled_bias = ...\n",
    "        return F.linear(x, scaled_weight, scaled_bias)\n",
    "    \n",
    "\n",
    "class EqualizedConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional layer with Equalized Learning Rate (EqLR)\n",
    "    Args:\n",
    "        input_channels: Number of input channels\n",
    "        output_channels: Number of output channels\n",
    "        kernel_size: Kernel size \n",
    "        stride: Stride of the convolution\n",
    "        gain: Activation scaling factor\n",
    "        lrmul: Learning rate multiplier\n",
    "        padding: Zero-padding\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, stride=1, gain=2**0.5, lrmul=1.0, padding=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calculate He initialization constant\n",
    "        # For Conv2d, fan_in = input_channels * kernel_width * kernel_height\n",
    "        fan_in = ...\n",
    "        he_std = ...\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.w_mul = he_std * lrmul\n",
    "        self.b_mul = lrmul\n",
    "        init_std = 1.0 / lrmul\n",
    "\n",
    "        # Initialize self.weight with N(0, 1) scaled by init_std\n",
    "        self.weight = nn.Parameter(...)\n",
    "        self.bias = nn.Parameter(torch.zeros(output_channels))\n",
    "            \n",
    "        self.stride = stride\n",
    "        self.padding = padding if padding is not None else kernel_size // 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Scale weights by self.w_mul and bias by self.b_mul\n",
    "        scaled_weight = ...\n",
    "        scaled_bias = ...\n",
    "        return F.conv2d(\n",
    "            x,\n",
    "            scaled_weight, \n",
    "            scaled_bias,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb2328e",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 3: Pixel Normalization Layer, NoiseLayer (1 –±–∞–ª–ª)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf32f5",
   "metadata": {},
   "source": [
    "#### Pixel Normalization Layer\n",
    "\n",
    "–ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã `GAN` —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å —Å —Ç–µ–º, —á—Ç–æ `Batch Normalization` –Ω–µ —Ä–∞–±–æ—Ç–∞–ª–∞ –¥–æ–ª–∂–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –∏–∑-–∑–∞ —Ç–æ–≥–æ, —á—Ç–æ —Ä–∞–∑–º–µ—Ä—ã –±–∞—Ç—á–µ–π —á–∞—Å—Ç–æ –±—ã–ª–∏ —Å–ª–∏—à–∫–æ–º –º–∞–ª—ã, –∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –Ω–∏–º ‚Äî —Å–ª–∏—à–∫–æ–º —à—É–º–Ω–æ–π.\n",
    "\n",
    "–ê–≤—Ç–æ—Ä—ã `ProGAN` –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ –ø–∏–∫—Å–µ–ª—è–º** (**Pixel Normalization**).\n",
    "\n",
    "–í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Å–µ–º—É –±–∞—Ç—á—É, –º—ã –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º $L_2$-–Ω–æ—Ä–º—É –≤–µ–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–¥–æ–ª—å –∫–∞–Ω–∞–ª–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–∏–∫—Å–µ–ª—è. –¢–æ –µ—Å—Ç—å, –¥–ª—è –∫–∞–∂–¥–æ–≥–æ $(x, y)$ –ø–∏–∫—Å–µ–ª—è –º—ã –±–µ—Ä–µ–º –µ–≥–æ –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ $C$ –∫–∞–Ω–∞–ª–∞–º –∏ –ø—Ä–∏–≤–æ–¥–∏–º –µ–≥–æ –¥–ª–∏–Ω—É –∫ –µ–¥–∏–Ω–∏—Ü–µ:\n",
    "\n",
    "$$b_{x,y} = \\frac{a_{x,y}}{\\sqrt{\\frac{1}{C} \\sum_{j=0}^{C-1} (a_{x,y}^j)^2 + \\epsilon}}$$\n",
    "\n",
    "–≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –∏ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–µ–Ω –≤ —Å–∞–º–æ–º –Ω–∞—á–∞–ª–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞, –≤ `Mapping Network`, –≥–¥–µ –æ–Ω –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –≤—Ö–æ–¥–Ω–æ–º—É –≤–µ–∫—Ç–æ—Ä—É $\\mathbf{z}$.\n",
    "\n",
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥ `forward` –¥–ª—è —Å–ª–æ—è `PixelNormLayer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3203d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNormLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Pixel Normalization\n",
    "    Used in the Mapping Network to normalize the magnitude of feature vectors\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Calculate Mean Square across the channel dimension (dim=1)\n",
    "        ms = ...\n",
    "\n",
    "        # Calculate the inverse Root Mean Square (rRMS), use torch.rsqrt() for computational efficiency\n",
    "\n",
    "        rrms = ...\n",
    "        \n",
    "        # Apply rRMS to the input tensor x\n",
    "        x_norm = ...\n",
    "        \n",
    "        return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42413ce",
   "metadata": {},
   "source": [
    "#### NoiseLayer\n",
    "\n",
    "–í –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö `GAN` –≤–µ–∫—Ç–æ—Ä $\\mathbf{z}$ –æ—Ç–≤–µ—á–∞–ª —Å—Ä–∞–∑—É –∏ –∑–∞ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –∏ –∑–∞ –º–µ–ª–∫–∏–µ –¥–µ—Ç–∞–ª–∏. –ü–æ—ç—Ç–æ–º—É, –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –±—ã–ª–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è, —Å–µ—Ç—å –ø—ã—Ç–∞–ª–∞—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏—Ö –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫. –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏–ª–æ –∫ —Ç–æ–º—É, —á—Ç–æ –º–µ–ª–∫–∏–µ –¥–µ—Ç–∞–ª–∏ –≤—ã–≥–ª—è–¥–µ–ª–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏–ª–∏ –∫ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –ª–∏—Ü–∞.\n",
    "\n",
    "–í `StyleGAN` —Ä–µ—à–∏–ª–∏ —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –≤–≤–µ–¥—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π **—Å–ª–æ–π —à—É–º–∞** (**Noise Layer**) –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è –≤ —Å–µ—Ç–∏ —Å–∏–Ω—Ç–µ–∑–∞.\n",
    "\n",
    "–≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—É —Ä–∞–∑–¥–µ–ª–∏—Ç—å –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏:\n",
    "\n",
    "- **–í–µ–∫—Ç–æ—Ä —Å—Ç–∏–ª—è** $\\mathbf{w}$: –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã (–ø–æ–∑–∞, –≤–æ–∑—Ä–∞—Å—Ç)\n",
    "\n",
    "- **–°–ª–æ–π —à—É–º–∞**: –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –º–µ–ª–∫–∏–µ –¥–µ—Ç–∞–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –≤–ª–∏—è—é—Ç –Ω–∞ –æ–±—â—É—é –∫–∞—Ä—Ç–∏–Ω—É\n",
    "\n",
    "–®—É–º –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–∞–µ–º—ã—Ö –≤–µ—Å–æ–≤ `self.weight`. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–µ—Ç–∏ —Ä–µ—à–∞—Ç—å, —Å–∫–æ–ª—å–∫–æ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞.\n",
    "\n",
    "–í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ `forward` –≤ —Å–ª–æ–µ `NoiseLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b63836",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Injects per-pixel Gaussian noise, scaled by a learned per-channel weight.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(channels))\n",
    "        self.noise = None \n",
    "\n",
    "    def forward(self, x, noise=None):\n",
    "        # x shape: [B, C, H, W]\n",
    "        \n",
    "        if noise is None and self.noise is None:\n",
    "            # Create the noise tensor using torch.randn (noise must be 1-channel for broadcasting)\n",
    "            noise = ...\n",
    "\n",
    "        elif noise is None:\n",
    "            noise = self.noise\n",
    "        \n",
    "        # Scale the noise by the learned weight and add to features\n",
    "       \n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74f09ec",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 4: AdaIN (1 –±–∞–ª–ª)\n",
    "\n",
    "–í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –ø—Ä–æ—Å—Ç–æ –ø–æ–¥–∞–≤–∞—Ç—å –≤–µ–∫—Ç–æ—Ä —Å—Ç–∏–ª—è $\\mathbf{w}$ –≤ –Ω–∞—á–∞–ª–µ, `StyleGAN` –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–≥–æ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —É—Ä–æ–≤–Ω–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –≠—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∑–∞ —Å—á–µ—Ç **–∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏** (**Adaptive Instance Normalization**, **AdaIN**).\n",
    "\n",
    "AdaIN —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —ç—Ç–∞–ø–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º —Å –ø–æ–º–æ—â—å—é –¥–≤—É—Ö –∫–ª–∞—Å—Å–æ–≤:\n",
    "\n",
    "- **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è** $\\mathbf{x}_i \\to \\mathbf{x}'$: –°–Ω–∞—á–∞–ª–∞ —Å—Ç–∏—Ä–∞–µ—Ç—Å—è —Å—Ç–∞—Ä—ã–π —Å—Ç–∏–ª—å –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø—Ä–∏—á–µ–º –∫–∞–∂–¥–∞—è –∫–∞—Ä—Ç–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ $\\mathbf{x}_i$ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ. –≠—Ç–æ—Ç —ç—Ç–∞–ø —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é `Instance Normalization` –≤–Ω—É—Ç—Ä–∏ `StyleBlock`.\n",
    "\n",
    "- **–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è** ($\\mathbf{w} \\to \\mathbf{y} \\to \\mathbf{b}$): –ó–∞—Ç–µ–º –º—ã –≤–Ω–µ–¥—Ä—è–µ–º –Ω–æ–≤—ã–π —Å—Ç–∏–ª—å. –í–µ–∫—Ç–æ—Ä $\\mathbf{w}$ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –æ–±—É—á–∞–µ–º—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –∏ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ –≤–µ–∫—Ç–æ—Ä —Å—Ç–∏–ª—è $\\mathbf{y} = (y_s, y_b)$. –≠—Ç–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è ($y_{s,i}$) –∏ —Å–¥–≤–∏–≥–∞ ($y_{b,i}$) –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ $\\mathbf{x}'$. –≠—Ç–æ—Ç —ç—Ç–∞–ø —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –≤ –∫–ª–∞—Å—Å–µ `StyleModulation`.\n",
    "\n",
    "–§–æ—Ä–º—É–ª–∞, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∞—è —ç—Ç—É –æ–ø–µ—Ä–∞—Ü–∏—é:\n",
    "\n",
    "$$\\text{AdaIN}(\\mathbf{x}_i, \\mathbf{y}) = y_{s,i} \\frac{\\mathbf{x}_i - \\mu(\\mathbf{x}_i)}{\\sigma(\\mathbf{x}_i)} + y_{b,i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b967e3",
   "metadata": {},
   "source": [
    "#### StyleModulation\n",
    "\n",
    "–≠—Ç–æ—Ç –∫–ª–∞—Å—Å –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é. –û–Ω –±–µ—Ä–µ—Ç –≤–µ–∫—Ç–æ—Ä $\\mathbf{w}$, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –µ–≥–æ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ç–∏–ª—è $y_{s,i}$ –∏ $y_{b,i}$ –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∏—Ö –∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –∫–∞—Ä—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "\n",
    "–í–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –∫–ª–∞—Å—Å–µ `StyleModulation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64225169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleModulation(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies the scale (ys) and bias (yb) parameters derived from w.\n",
    "    This performs the Adaptive part of the AdaIN operation.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_size, channels): \n",
    "        super().__init__()\n",
    "        self.lin = EqualizedLinear(latent_size, channels * 2, gain=1.0, lrmul=1.0)\n",
    "\n",
    "    def forward(self, x, latent):\n",
    "        style = self.lin(latent)\n",
    "        \n",
    "        # Reshape style into [B, 2 (scale/bias), C, 1, 1] for broadcasting\n",
    "        shape = [-1, 2, x.size(1)] + (x.dim() - 2) * [1]\n",
    "        style = style.view(shape)\n",
    "        \n",
    "        # get ys and yb (ys corresponds to style[:, 0], yb to style[:, 1])\n",
    "        ys = ...\n",
    "        yb = ...\n",
    "        \n",
    "        #Apply the final AdaIN formula\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5474ddd9",
   "metadata": {},
   "source": [
    "#### StyleBlock\n",
    "\n",
    "–≠—Ç–æ—Ç –∫–ª–∞—Å—Å —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –æ–ø–µ—Ä–∞—Ü–∏–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–µ. –ï–≥–æ –æ—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ‚Äî –æ–±–µ—Å–ø–µ—á–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: \n",
    "\n",
    "`Noise` $\\to$ `Activation` $\\to$ `InstanceNorm` $\\to$ `StyleAdjustment`. \n",
    "\n",
    "`Instance Norm` —Å—Ç–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ä—ã–π —Å—Ç–∏–ª—å, –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—è –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫ –Ω–æ–≤–æ–º—É —Å—Ç–∏–ª—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The full synthesis pipeline: Noise -> Activation -> Norm -> StyleAdjustment.\n",
    "    Args:\n",
    "        channels: Number of feature channels in the input map\n",
    "        dlatent_size: Dimensionality of the intermediate latent space\n",
    "        activation_layer: The activation function to use\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, dlatent_size, activation_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        \n",
    "        layers.append(('noise', NoiseLayer(channels)))\n",
    "        layers.append(('activation', activation_layer))\n",
    "        layers.append(('instance_norm', nn.InstanceNorm2d(channels)))\n",
    "\n",
    "        self.main_ops = nn.Sequential(OrderedDict(layers))\n",
    "        self.style_mod = StyleModulation(latent_size=dlatent_size, channels=channels) \n",
    "\n",
    "    def forward(self, x, dlatents_in_slice=None):\n",
    "        \"\"\"\n",
    "        Processes the feature map and applies style adjustment.\n",
    "        :param x: The input feature map\n",
    "        :param dlatents_in_slice: A slice of the W vector containing the style parameters for this layer \n",
    "        \"\"\"\n",
    "        # Get x_norm using main_ops\n",
    "        x_norm = ...\n",
    "        \n",
    "        # Apply Style Modulation\n",
    "        x = ...\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc785f78",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 5: Minibatch Standard Deviation (0.5 –±–∞–ª–ª–∞)\n",
    "\n",
    "–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö `GAN` —è–≤–ª—è–µ—Ç—Å—è **–∫–æ–ª–ª–∞–ø—Å –º–æ–¥** (**Mode Collapse**) ‚Äî –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –Ω–∞—Ö–æ–¥–∏—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —É—Å–ø–µ—à–Ω–æ –æ–±–º–∞–Ω—ã–≤–∞—é—Ç –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä, –∞ –∑–∞—Ç–µ–º —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–µ –æ–±—ä–µ–∫—Ç—ã —Ç–æ–ª—å–∫–æ –∏–∑ —ç—Ç–æ–π –º–æ–¥—ã.\n",
    "\n",
    "–û–±—ã—á–Ω—ã–π –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –¥—Ä—É–≥–∏—Ö –≤ –±–∞—Ç—á–µ, –ø–æ—ç—Ç–æ–º—É –æ–Ω –Ω–µ –º–æ–∂–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–µ–Ω –≤–µ—Å—å –±–∞—Ç—á —Ñ–µ–π–∫–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. \n",
    "\n",
    "–ò–¥–µ—è **Minibatch Standard Deviation** (**MBSD**) –±—ã–ª–∞ –≤–ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ 2016 –≥–æ–¥—É [Salimans et al.](https://arxiv.org/pdf/1606.03498) –∏ –±—ã–ª–∞ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ `ProGAN`. **MBSD** —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–ª–ª–∞–ø—Å–∞ –º–æ–¥, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä—É –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –º–∏–Ω–∏-–±–∞—Ç—á–∞.\n",
    "\n",
    "**MBSD** —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –¥–µ—Ç–µ–∫—Ç–æ—Ä –æ–¥–Ω–æ–æ–±—Ä–∞–∑–∏—è, –∫–æ—Ç–æ—Ä—ã–π –≥–æ–≤–æ—Ä–∏—Ç –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä—É, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –±–∞—Ç—á–µ:\n",
    "\n",
    "- –°–ª–æ–π –¥–µ–ª–∏—Ç –º–∏–Ω–∏-–±–∞—Ç—á –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏–µ –≥—Ä—É–ø–ø—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "\n",
    "- –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ $\\mathbf{x}$ –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ $\\boldsymbol{\\sigma}$ –ø–æ –æ—Å–∏ –±–∞—Ç—á–∞. –≠—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –ø—Ä–∏–∑–Ω–∞–∫ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö\n",
    "\n",
    "- –í—Å–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è $\\boldsymbol{\\sigma}$ —É—Å—Ä–µ–¥–Ω—è—é—Ç—Å—è –ø–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º –∏ –∫–∞–Ω–∞–ª–∞–º\n",
    "\n",
    "- –≠—Ç–æ—Ç –≤–µ–∫—Ç–æ—Ä –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫–∞–∫ –Ω–æ–≤—ã–π –∫–∞–Ω–∞–ª –∫ –≤—ã—Ö–æ–¥–Ω–æ–π –∫–∞—Ä—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞\n",
    "\n",
    "–í–∞–º –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —à–∞–≥–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è $\\boldsymbol{\\sigma}$, –æ–ø–∏—Å–∞–Ω–Ω—ã–µ –≤—ã—à–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9db235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchStddev(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates the standard deviation of features across the minibatch group \n",
    "    and appends it as a new feature map to discourage mode collapse.\n",
    "    Args:\n",
    "        group_size: The number of samples to group together for statistics, must be a divisor of the batch size \n",
    "        num_new_features: Number of new feature maps to append\n",
    "    \"\"\"\n",
    "    def __init__(self, group_size=4, num_new_features=1):\n",
    "        super().__init__()\n",
    "        self.group_size = group_size\n",
    "        self.num_new_features = num_new_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        group_size = min(self.group_size, b)\n",
    "        \n",
    "        # Reshape to [G, M, F, C', H, W] where G is group_size, M is Minibatches, F is num_new_features, C' is Internal channels\n",
    "        y = x.view(group_size, -1, self.num_new_features, c // self.num_new_features, h, w)\n",
    "        \n",
    "        # Calculate the standard deviation sqrt(Mean((y - mean(y))^2) + epsilon)\n",
    "        y_std = ...\n",
    "\n",
    "        # Average Std Dev over all non-group dimensions (C', H, W)\n",
    "        y_avg = ...\n",
    "        \n",
    "        # Replicate y_avg over the group dimension and expand to [B, F, H, W]\n",
    "        y_final = ...\n",
    "        \n",
    "        #  Concatenate x and y along the channel dimension\n",
    "        z = ...\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375503b",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 6: Truncation Trick (0.5 –±–∞–ª–ª–∞)\n",
    "\n",
    "–û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ª—É—á—à–µ –≤—Å–µ–≥–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤—ã—Å–æ–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, —Ç–æ –µ—Å—Ç—å, —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã—Ö –∏–ª–∏ \"—Ç–∏–ø–∏—á–Ω—ã—Ö\" –ª–∏—Ü. –û–±–ª–∞—Å—Ç–∏ —Å –Ω–∏–∑–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –Ω–µ—Ç–∏–ø–∏—á–Ω—ã–µ, —Å–∏–ª—å–Ω–æ –∏—Å–∫–∞–∂–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã.\n",
    "\n",
    "**–¢—Ä—é–∫ —É—Å–µ—á–µ–Ω–∏—è** (**Truncation Trick**) ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ï–≥–æ —Ü–µ–ª—å ‚Äî –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –ª—é–±–æ–π —Å–ª—É—á–∞–π–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —Å—Ç–∏–ª—è $\\mathbf{w}$ –∏–∑ –æ–±–ª–∞—Å—Ç–µ–π —Å –Ω–∏–∑–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é –æ–±—Ä–∞—Ç–Ω–æ –∫ —Å—Ä–µ–¥–Ω–µ–º—É –≤–µ–∫—Ç–æ—Ä—É —Å—Ç–∏–ª—è $\\bar{\\mathbf{w}}$. \n",
    "\n",
    "–° –æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, —ç—Ç–æ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏ —É–º–µ–Ω—å—à–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤, –∞ —Å –¥—Ä—É–≥–æ–π ‚Äî —Å–Ω–∏–∂–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤.\n",
    "\n",
    "–¢—Ä—é–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n",
    "\n",
    "- –í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –º—ã —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ $\\bar{\\mathbf{w}}$ –≤—Å–µ—Ö —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å—Ç–∏–ª—è $\\mathbf{w}$.\n",
    "\n",
    "- –ü—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä $\\mathbf{w}$ —Å–º–µ—à–∏–≤–∞–µ—Ç—Å—è —Å $\\bar{\\mathbf{w}}$ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ $\\psi$ (`threshold`):\n",
    "\n",
    "$$\\mathbf{w}' = \\bar{\\mathbf{w}} + \\psi \\cdot (\\mathbf{w} - \\bar{\\mathbf{w}})$$\n",
    "\n",
    "–í `StyleGAN` —ç—Ç–æ—Ç —Ç—Ä—é–∫ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫ –ø–µ—Ä–≤—ã–º `max_layer` –≤–µ–∫—Ç–æ—Ä–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—é—Ç –≥—Ä—É–±—ã–µ, –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã.\n",
    "\n",
    "–í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ª–æ–≥–∏–∫—É **Truncation Trick**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Truncation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Truncation Trick in W-space to bias random samples \n",
    "    towards the mean, improving fidelity at the cost of diversity.\n",
    "    \"\"\"\n",
    "    def __init__(self, avg_latent, max_layer=8, threshold=0.7, beta=0.995):\n",
    "        super().__init__()\n",
    "        self.max_layer = max_layer\n",
    "        self.threshold = threshold\n",
    "        self.beta = beta\n",
    "        self.register_buffer('avg_latent', avg_latent)\n",
    "\n",
    "    def update(self, last_avg):\n",
    "        \"\"\"\n",
    "        Calculates the new exponential moving average (EMA) of the W vector.\n",
    "        :param last_avg: The mean W vector of the current training batch.\n",
    "        \"\"\"\n",
    "        # Implement the EMA : new_avg = beta * old_avg + (1 - beta) * current_avg\n",
    "        self.avg_latent.copy_(...) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply Truncation formula, use torch.lerp\n",
    "        interp = ...\n",
    "        \n",
    "        # Create the masking tensor (only for layers < max_layer)\n",
    "        do_trunc = ...\n",
    "        \n",
    "        # Apply the mask\n",
    "\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cdfc10",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 7: Constant Input Block (0.5 –±–∞–ª–ª–∞)\n",
    "\n",
    "–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π, –≥–¥–µ —Å–∫—Ä—ã—Ç—ã–π –≤–µ–∫—Ç–æ—Ä $\\mathbf{z}$ –Ω–∞–ø—Ä—è–º—É—é –ø–æ–¥–∞–≤–∞–ª—Å—è –≤ –ø–µ—Ä–≤—ã–π —Å–ª–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞, `StyleGAN` –Ω–∞—á–∏–Ω–∞–µ—Ç —Å–∏–Ω—Ç–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ —Å —à—É–º–∞, –∞ —Å –æ–±—É—á–∞–µ–º–æ–π –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã. –ó–∞ —ç—Ç–æ –æ—Ç–≤–µ—á–∞–µ—Ç –∫–ª–∞—Å—Å `ConstantInputBlock`.\n",
    "\n",
    "–û–±—É—á–∞–µ–º–∞—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ `self.const` –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–æ–ª—å —Ö–æ–ª—Å—Ç–∞ ‚Äî –æ–±—É—á–∞–µ–º–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ —Ä–∞–∑–º–µ—Ä–∞ $4 \\times 4 \\times 512$, –∫–æ—Ç–æ—Ä—ã–π –¥–∞—ë—Ç –Ω–∞—á–∞–ª—å–Ω—É—é –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ü–æ—Å–∫–æ–ª—å–∫—É —ç—Ç–æ—Ç —Ö–æ–ª—Å—Ç —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—É –Ω–µ –Ω—É–∂–Ω–æ —Ç—Ä–∞—Ç–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –Ω–∞ –µ—ë —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ –≤–µ–∫—Ç–æ—Ä–∞ $\\mathbf{z}$.\n",
    "\n",
    "–ó–∞–¥–∞—á–∞ `ConstantInputBlock` —Å–≤–æ–¥–∏—Ç—Å—è –∫ —Ç–æ–º—É, —á—Ç–æ–±—ã –≤–∑—è—Ç—å —ç—Ç–æ—Ç –æ–±—É—á–∞–µ–º—ã–π —Ç–µ–Ω–∑–æ—Ä –∏ –Ω–∞—á–∞—Ç—å –µ–≥–æ —Å—Ç–∏–ª–∏–∑–∞—Ü–∏—é:\n",
    "\n",
    "- –°–Ω–∞—á–∞–ª–∞ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ç–µ–Ω–∑–æ—Ä—É –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ø–µ—Ä–≤—ã–π –≤–µ–∫—Ç–æ—Ä —Å—Ç–∏–ª—è $\\mathbf{w}_0$, —á—Ç–æ –∑–∞–¥–∞–µ—Ç –≥—Ä—É–±—ã–µ, –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "\n",
    "- –ó–∞—Ç–µ–º —Å–ª–µ–¥—É–µ—Ç —Å–≤–µ—Ä—Ç–∫–∞, –∞ –ø–æ—Å–ª–µ –Ω–µ–µ ‚Äî –≤—Ç–æ—Ä–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ —Å—Ç–∏–ª—è $\\mathbf{w}_1$\n",
    "\n",
    "–í–∞–º –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ª–æ–≥–∏–∫—É `ConstantInputBlock`, –∑–∞–ø–æ–ª–Ω–∏–≤ –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –∫–æ–¥–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantInputBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The initial 4x4 block of the StyleGAN Generator. \n",
    "    It replaces the direct Z input with a trainable constant\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_channels, \n",
    "                 dlatent_size, \n",
    "                 activation_layer):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize self.const with a tensor of ones\n",
    "        self.const = nn.Parameter(...) \n",
    "        \n",
    "        # We also need a bias parameter for the constant input\n",
    "        self.bias = nn.Parameter(...)\n",
    "        \n",
    "        self.style_block1 = StyleBlock(\n",
    "            channels=num_channels, \n",
    "            dlatent_size=dlatent_size, \n",
    "            activation_layer=activation_layer\n",
    "        )\n",
    "        \n",
    "        self.conv = EqualizedConv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.style_block2 = StyleBlock(\n",
    "            channels=num_channels, \n",
    "            dlatent_size=dlatent_size,  \n",
    "            activation_layer=activation_layer\n",
    "        )\n",
    "\n",
    "    def forward(self, w_slice):\n",
    "        # w_slice contains the first two W vectors: [B, 2, D]\n",
    "        batch_size = w_slice.size(0)\n",
    "\n",
    "        # Expand the constant to match the batch size [1, C, 4, 4] -> [B, C, 4, 4]\n",
    "        x = ...\n",
    "        \n",
    "        # Add the learned bias\n",
    "        x = ...\n",
    "\n",
    "        # Apply the first style w_0\n",
    "        x = ...\n",
    "        \n",
    "        # Pass through convolution\n",
    "        x = ...\n",
    "        \n",
    "        # Apply the second style w1\n",
    "        x = ...\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e686a0",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 8: Synthesis Block (0.5 –±–∞–ª–ª–∞)\n",
    "\n",
    "`SynthesisBlock` ‚Äî —ç—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π –±–ª–æ–∫, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏ –≤—ã–¥–∞–µ—Ç –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–¥–≤–æ–µ –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.\n",
    "\n",
    "–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç `ConstantInputBlock`, –∫–æ—Ç–æ—Ä—ã–π —Ç–æ–ª—å–∫–æ —Å–æ–∑–¥–∞–µ—Ç –∑–∞–≥–æ—Ç–æ–≤–∫—É $4 \\times 4$, —ç—Ç–æ—Ç –±–ª–æ–∫ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º—Å—è —Ü–∏–∫–ª–æ–º —Å–∏–Ω—Ç–µ–∑–∞, —Å–æ—Å—Ç–æ—è—â–∏–º –∏–∑ 3 —ç—Ç–∞–ø–æ–≤:\n",
    "\n",
    "1. **Upsampling**\n",
    "\n",
    "2. **First Convolution and Style Adjustment**\n",
    "\n",
    "3. **Second Convolution and Final Style Adjustment**\n",
    "\n",
    "–ó–∞–º–µ—Ç—å—Ç–µ, —á—Ç–æ –±–ª–æ–∫ –ø–æ—Ç—Ä–µ–±–ª—è–µ—Ç –¥–≤–∞ –≤–µ–∫—Ç–æ—Ä–∞ —Å—Ç–∏–ª—è –Ω–∞ –æ–¥–Ω–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ. –≠—Ç–æ —Å–¥–µ–ª–∞–Ω–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã —É–≤–µ–ª–∏—á–∏—Ç—å –µ–º–∫–æ—Å—Ç—å –∏ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å —Å–µ—Ç–∏. –ü–µ—Ä–≤—ã–π –≤–µ–∫—Ç–æ—Ä —Å—Ç–∏–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –ø–æ–≤—ã—à–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è, –∞ –≤—Ç–æ—Ä–æ–π ‚Äî —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏. \n",
    "\n",
    "–ë–ª–∞–≥–æ–¥–∞—Ä—è —Ç–∞–∫–æ–º—É **Style Mixing** –º—ã –º–æ–∂–µ–º —Å–º–µ—à–∏–≤–∞—Ç—å –∞—Ç—Ä–∏–±—É—Ç—ã, –≤–∑—è—Ç—ã–µ —Å —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2eeab4",
   "metadata": {},
   "source": [
    "#### Upsample, DownSample\n",
    "\n",
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ `StyleGAN` –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –Ω–∞ –∏–¥–µ–µ **Progressive Growing**, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–π **–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä** –∏ **–¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä** —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –ê –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –º–µ–∂–¥—É —ç—Ç–∏–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏–∑–º–µ–Ω–µ–Ω—è—Ç—å —Ä–∞–∑–º–µ—Ä—ã –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n",
    "\n",
    "- **Upsampling**: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "\n",
    "- **Downsampling**: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–æ–º –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "\n",
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∏ —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ `PyTorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthesisBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The main repeating block of the Generator pipeline. \n",
    "    Doubles the resolution and consumes two W vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dlatent_size, gain, activation_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        self.conv1 = EqualizedConv2d(in_channels, \n",
    "                                     out_channels, \n",
    "                                     kernel_size=3, \n",
    "                                     padding=1, \n",
    "                                     gain=gain\n",
    "                                     )\n",
    "        \n",
    "        self.style_block1 = StyleBlock(channels=out_channels, \n",
    "                                       dlatent_size=dlatent_size,  \n",
    "                                       activation_layer=activation_layer\n",
    "                                       )\n",
    "        \n",
    "        self.conv2 = EqualizedConv2d(out_channels, \n",
    "                                     out_channels, \n",
    "                                     kernel_size=3, \n",
    "                                     padding=1, \n",
    "                                     gain=gain\n",
    "                                     )\n",
    "        self.style_block2 = StyleBlock(channels=out_channels, \n",
    "                                       dlatent_size=dlatent_size, \n",
    "                                       activation_layer=activation_layer\n",
    "                                       )\n",
    "\n",
    "    def forward(self, x, w_slice):\n",
    "        \n",
    "        # Apply upsample to the input x\n",
    "        x = ... \n",
    "        # Apply conv1 to the result\n",
    "        x = ...\n",
    "        # Apply self.style_block1, using the first w vector slice\n",
    "        x = ...\n",
    "        # Apply self.conv2\n",
    "        x = ...\n",
    "        # Apply self.style_block2, using the second w vector slice\n",
    "        x = ...\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f7209",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 9: Discriminator Output Block (0.5 –±–∞–ª–ª–∞)\n",
    "\n",
    "`DiscriminatorOutputBlock` —è–≤–ª—è–µ—Ç—Å—è —Ñ–∏–Ω–∞–ª—å–Ω—ã–º —ç—Ç–∞–ø–æ–º —Ä–∞–±–æ—Ç—ã –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞,  –≤—ã–ø–æ–ª–Ω—è—è —Ç—Ä–∏  –¥–µ–π—Å—Ç–≤–∏—è:\n",
    "\n",
    "1. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ —Å–ª–æ–π `MinibatchStddev` –∏ –ø–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–∏ –≤—Å–µ–≥–æ –±–∞—Ç—á–∞\n",
    "\n",
    "2. –í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–≤–µ—Ä—Ç–∫—É $3 \\times 3$, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ \n",
    "\n",
    "3. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–≤–µ—Ä—Ç–∫–∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –¥–≤–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ—è, –∫–æ—Ç–æ—Ä—ã–µ –∏ –≤—ã–¥–∞—é—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "\n",
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥ `forward` –≤ –∫–ª–∞—Å—Å–µ `DiscriminatorOutputBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31881435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorOutputBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Final block of the Discriminator. It applies MBSD, the last feature \n",
    "    extraction, and converts the 4x4 feature map into a single realism score.\n",
    "\n",
    "    Args:\n",
    "            mbstd_group_size (int): Number of samples per group for Minibatch Standard Deviation \n",
    "            mbstd_num_features (int): Number of new feature maps to append from MBSD\n",
    "            in_channels (int): Number of input channels to this block\n",
    "            intermediate_channels (int): Number of channels in the first fully connected layer\n",
    "            gain (float): He initialization gain for most layers\n",
    "            activation_layer (nn.Module): The activation function\n",
    "            resolution (int): The spatial resolution of the input feature map\n",
    "            in_channels2 (int, optional): Output channels of the last convolution, defaults to in_channels\n",
    "            output_features (int): Final output size \n",
    "            last_gain (float): Gain for the final output linear layer\n",
    "    \"\"\"\n",
    "    def __init__(self, mbstd_group_size, \n",
    "                 mbstd_num_features, \n",
    "                 in_channels, \n",
    "                 intermediate_channels, \n",
    "                 gain, \n",
    "                 activation_layer, \n",
    "                 resolution=4, \n",
    "                 in_channels2=None, \n",
    "                 output_features=1, \n",
    "                 last_gain=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if in_channels2 is None:\n",
    "            in_channels2 = in_channels\n",
    "            \n",
    "        if mbstd_group_size > 1:\n",
    "            self.mbstd = MinibatchStddev(mbstd_group_size, mbstd_num_features)\n",
    "            final_in_channels = in_channels + mbstd_num_features\n",
    "        else:\n",
    "            self.mbstd = None\n",
    "            final_in_channels = in_channels\n",
    "\n",
    "        self.conv = EqualizedConv2d(final_in_channels, in_channels2, kernel_size=3, padding=1, gain=gain)\n",
    "        self.act1 = activation_layer\n",
    "        self.fc1 = EqualizedLinear(in_channels2 * resolution * resolution, intermediate_channels, gain=gain)\n",
    "        self.act2 = activation_layer\n",
    "        self.fc2 = EqualizedLinear(intermediate_channels, output_features, gain=last_gain)\n",
    "\n",
    "\n",
    "    def forward(self, x):    \n",
    "\n",
    "        if self.mbstd is not None:\n",
    "            # Apply self.mbstd to the input x\n",
    "            x = ...\n",
    "            \n",
    "        # Apply activation to the result of the convolution\n",
    "        x = self.act1(self.conv(x))\n",
    "        \n",
    "        # Flatten the tensor using torch.flatten()\n",
    "        x_flat = ...\n",
    "        \n",
    "        # Apply the first linear layer, then the second activation\n",
    "        x = ...\n",
    "        \n",
    "        # Return the result of the last linear layer\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3981f2",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 10: Discriminator Block (0.5 –±–∞–ª–ª–∞)\n",
    "`DiscriminatorBlock` ‚Äî —ç—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å –∫—Ä–∏—Ç–∏–∫–∞:\n",
    "\n",
    "- –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è\n",
    "\n",
    "- –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –µ—ë —Å–≤–µ—Ä—Ç–∫–∞–º–∏\n",
    "\n",
    "- –£–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –≤ 2 —Ä–∞–∑–∞\n",
    "\n",
    "–ë–ª–æ–∫ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤ –∏ –æ–ø–µ—Ä–∞—Ü–∏–∏ **—É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è** (**Downsampling**). –î–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `Average Pooling`.\n",
    "\n",
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ `DiscriminatorBlock`, –∑–∞–ø–æ–ª–Ω–∏–≤ –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –∫–æ–¥–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe8ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The main repeating block of the Discriminator.\n",
    "    It extracts features and then downsamples the resolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 gain, \n",
    "                 activation_layer):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = EqualizedConv2d(in_channels, in_channels, kernel_size=3, padding=1, gain=gain)\n",
    "        self.act1 = activation_layer\n",
    "     \n",
    "        self.conv2 = EqualizedConv2d(in_channels, out_channels, kernel_size=3, padding=1, gain=gain)\n",
    "        self.act2 = activation_layer\n",
    "        \n",
    "        # Initialize nn.AvgPool2d\n",
    "        self.downsample = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass x through conv1 and activation\n",
    "        x = ...\n",
    "        \n",
    "        # Pass x through conv2 and activation\n",
    "        x = ...\n",
    "        \n",
    "        # Apply downsampling\n",
    "        x = ...\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc14ae2",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 11: Mapping Network (0.5 –±–∞–ª–ª–∞)\n",
    "\n",
    "–í –æ–±—ã—á–Ω—ã—Ö `GAN`-–∞—Ö –≤—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä $\\mathbf{z}$ –±–µ—Ä–µ—Ç—Å—è –∏–∑ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})$. \n",
    "\n",
    "–ê–≤—Ç–æ—Ä—ã `StyleGAN` –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–µ –ø–æ–¥–∞–≤–∞—Ç—å $\\mathbf{z}$ —Å—Ä–∞–∑—É –≤ –º–æ–¥–µ–ª—å, –∞ —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –µ–≥–æ —á–µ—Ä–µ–∑ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—É—é —Å–µ—Ç—å ‚Äî `Mapping Network`, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏—Å–∫—Ä–∏–≤–ª–µ–Ω–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ $\\mathcal{Z}$ –≤ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ, ¬´—Ä–∞—Å–ø—É—Ç–∞–Ω–Ω–æ–µ¬ª –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ $\\mathcal{W}$.\n",
    "\n",
    "–í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `Mapping Network`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a0550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Maps the entangled input latent Z to the disentangled intermediate latent w\n",
    "\n",
    "    Args:\n",
    "        latent_size: Dim of the input latent vector z\n",
    "        dlatent_size: Dim of the output style vector w\n",
    "        num_layers_broadcast: If defined, broadcasts the output w for the synthesis network.\n",
    "        mapping_layers: Number of layers in the MLP\n",
    "        mapping_fmaps: Number of hidden channels in the MLP layers\n",
    "        mapping_lrmul: Learning rate multiplier\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 latent_size=512,          \n",
    "                 dlatent_size=512,          \n",
    "                 num_layers_broadcast=None, \n",
    "                 mapping_layers=8,         \n",
    "                 mapping_fmaps=512,        \n",
    "                 mapping_lrmul=0.01,\n",
    "                 **kwargs): \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers_broadcast = num_layers_broadcast\n",
    "        self.pixel_norm = PixelNormLayer()\n",
    "        self.act = nn.LeakyReLU(negative_slope=0.2)\n",
    "        gain = np.sqrt(2)\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(EqualizedLinear(\n",
    "            latent_size, mapping_fmaps, \n",
    "            gain=gain, lrmul=mapping_lrmul\n",
    "        ))\n",
    "        \n",
    "        for _ in range(1, mapping_layers - 1):\n",
    "            self.layers.append(EqualizedLinear(\n",
    "                mapping_fmaps, mapping_fmaps, \n",
    "                gain=gain, lrmul=mapping_lrmul\n",
    "            ))\n",
    "            \n",
    "        self.layers.append(EqualizedLinear(\n",
    "            mapping_fmaps, dlatent_size, \n",
    "            gain=gain, lrmul=mapping_lrmul\n",
    "        ))\n",
    "\n",
    "    def forward(self, z):\n",
    "        \n",
    "        # Apply PixelNorm to z \n",
    "        z = ...\n",
    "            \n",
    "        # Run through the MLP\n",
    "        for layer in self.layers:\n",
    "            # Apply linear layer and activation\n",
    "            w = ...\n",
    "            \n",
    "        if self.num_layers_broadcast is not None:\n",
    "            # Unsqueeze and expand 'w' to match the number of synthesis layers\n",
    "            # Target shape: [Batch, Num_Layers_Broadcast, Dlatent_Size]\n",
    "            w = ...\n",
    "            \n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9d1b7b",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 12: Synthesis Network (1 –±–∞–ª–ª)\n",
    "\n",
    "–°–µ—Ç—å —Å–∏–Ω—Ç–µ–∑–∞ –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—É—Ç–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ —Å—Ç–∏–ª—è $\\mathbf{w}$ –≤ –∏—Ç–æ–≥–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.\n",
    "\n",
    "–î–ª—è —ç—Ç–æ–≥–æ `StyleGAN` —É–Ω–∞—Å–ª–µ–¥–æ–≤–∞–ª –æ—Ç `ProGAN` —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è:\n",
    "\n",
    "- –û–±—É—á–∞–µ–º —Å–µ—Ç—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è $4 \\times 4$\n",
    "\n",
    "- –ö–æ–≥–¥–∞ –æ–±—É—á–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è, –º—ã –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –±–ª–æ–∫–∏ $8 \\times 8$, $16 \\times 16$ –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ\n",
    "\n",
    "–ü—Ä–æ—Ü–µ—Å—Å —Ä–æ—Å—Ç–∞ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω —á–µ—Ä–µ–∑ –¥–≤–µ –≤–µ—Ç–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–º–µ—à–∏–≤–∞—é—Ç—Å—è —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º $\\alpha$ (**Fade-in**):\n",
    "\n",
    "- **Residual Branch**: –±–µ—Ä–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ, —É–∂–µ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏ –ø—Ä–æ—Å—Ç–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –µ–≥–æ (**Upsample**), –º–∏–Ω—É—è –Ω–æ–≤—ã–µ —Å–ª–æ–∏.\n",
    "\n",
    "- **Straight Branch**: –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–µ, –æ–±—É—á–∞–µ–º—ã–µ –±–ª–æ–∫–∏ —Å–∏–Ω—Ç–µ–∑–∞.\n",
    "\n",
    "–í –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è $\\alpha \\approx 0$ –∏ –º—ã –≤–∏–¥–∏–º —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ä—ã–π —Å–ª–æ–π. –ü–æ –º–µ—Ä–µ –æ–±—É—á–µ–Ω–∏—è $\\alpha \\to 1$, –∏ –º—ã –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –Ω–æ–≤—ã–µ —Å–ª–æ–∏.\n",
    "\n",
    "–í–∞–º –Ω—É–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å `Synthesis Network` –∏–∑ —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthesisNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Constructs the image from w vectors using Progressive Growing mechanics.\n",
    "\n",
    "    Args:\n",
    "        dlatent_size: Dim of W\n",
    "        num_channels: Output image channels\n",
    "        resolution: Target output resolution\n",
    "        fmap_base: Base number of feature maps\n",
    "        fmap_decay: Decay rate for feature maps as resolution increases.\n",
    "        fmap_max: Maximum number of feature maps.\n",
    "        nonlinearity: Activation function name\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 dlatent_size=512, \n",
    "                 num_channels=3, \n",
    "                 resolution=128,\n",
    "                 fmap_base=8192, \n",
    "                 fmap_decay=1.0, \n",
    "                 fmap_max=512,\n",
    "                 nonlinearity='lrelu',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.fmap_base = fmap_base\n",
    "        self.fmap_decay = fmap_decay\n",
    "        self.fmap_max = fmap_max\n",
    "        \n",
    "        resolution_log2 = int(np.log2(resolution))\n",
    "        assert resolution == 2 ** resolution_log2 and resolution >= 4\n",
    "        self.depth = resolution_log2 - 1 \n",
    "\n",
    "        self.num_layers = (resolution_log2 - 1) * 2\n",
    "        self.num_styles = self.num_layers\n",
    "\n",
    "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
    "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
    "\n",
    "        \n",
    "        # Initialize ConstantInputBlock, use self._get_fmaps(1) for channels.\n",
    "        self.init_block = ...\n",
    "        \n",
    "        # We need a list of 1x1 convolutions to convert features to RGB at EACH resolution stage.\n",
    "        # Initialize the first RGB converter for 4x4 resolution\n",
    "        rgb_converters = [EqualizedConv2d(...)]\n",
    "\n",
    "        blocks = []\n",
    "        for res in range(3, resolution_log2 + 1):\n",
    "            last_channels = self._get_fmaps(res - 2)\n",
    "            channels = self._get_fmaps(res - 1)\n",
    "            \n",
    "            # Create and append a new SynthesisBlock for this resolution\n",
    "            blocks.append(SynthesisBlock(...))\n",
    "            \n",
    "            # Create and append a new RGB converter for this resolution\n",
    "            rgb_converters.append(EqualizedConv2d(...))\n",
    "\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.to_rgb = nn.ModuleList(rgb_converters)\n",
    "\n",
    "    def _get_fmaps(self, stage):\n",
    "        return min(\n",
    "            int(self.fmap_base / (2.0 ** (stage * self.fmap_decay))), \n",
    "            self.fmap_max\n",
    "        )\n",
    "\n",
    "    def forward(self, dlatents_in, depth=0, alpha=1.0):\n",
    "\n",
    "        assert depth < self.depth, \"Requested output depth cannot be produced\"\n",
    "\n",
    "        # Always run the 4x4 block first. It consumes W vectors at indices [0, 1]\n",
    "        x = self.init_block(dlatents_in[:, 0:2])\n",
    "        \n",
    "        if depth > 0:\n",
    "            # Run through all blocks that are BELOW the current new depth.\n",
    "            # These blocks are already fully trained.\n",
    "            for i, block in enumerate(self.blocks[:depth - 1]):\n",
    "                x = block(x, dlatents_in[:, 2 * (i + 1) : 2 * (i + 2)])\n",
    "            \n",
    "            # Upsample x (nearest) and apply the ToRGB layer from [depth-1]\n",
    "            residual = ...\n",
    "            residual = ...\n",
    "\n",
    "            # Apply the new SynthesisBlock and the new ToRGB layer\n",
    "            straight = ...\n",
    "            straight = ...\n",
    "            \n",
    "            # Mix straight and residual using alpha\n",
    "            images_out = ...\n",
    "        else:\n",
    "            # Depth 0 (4x4 only)\n",
    "            images_out = self.to_rgb[0](x)\n",
    "             \n",
    "        return images_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aded3d9",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 13: Generator (1 –±–∞–ª–ª)\n",
    "\n",
    "–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤ `StyleGAN` —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ–¥—Å–µ—Ç–µ–π:\n",
    "\n",
    "- **`Mapping Network`**: –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä $\\mathbf{z}$ –≤ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –ª–∞—Ç–µ–Ω—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä $\\mathbf{w}$\n",
    "- **`Synthesis Network`**: –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç $\\mathbf{w}$ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "\n",
    "#### Style Mixing \n",
    "\n",
    "–û–¥–Ω–æ–π –∏–∑ –≥–ª–∞–≤–Ω—ã—Ö –∏–¥–µ–π `StyleGAN` —è–≤–ª—è–µ—Ç—Å—è **—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é —Å–º–µ—à–∏–≤–∞–Ω–∏—è** (**Mixing Regularization**).\n",
    "\n",
    "–ï—Å–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤—Å–µ–≥–¥–∞ –æ–±—É—á–∞–µ—Ç—Å—è, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä $\\mathbf{w}$ –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤, –æ–Ω –º–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ –ª–æ–∂–Ω—ã–µ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ —Ä–∞–∑–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –æ–Ω –º–æ–∂–µ—Ç —Ä–µ—à–∏—Ç—å, –¥–ª–∏–Ω–Ω—ã–µ –≤–æ–ª–æ—Å—ã –≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤–º–µ—Å—Ç–µ —Å –∂–µ–Ω—Å–∫–∏–º –ª–∏—Ü–æ–º. –ò–∑-–∑–∞ —ç—Ç–æ–π –∂–µ—Å—Ç–∫–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –æ–¥–∏–Ω –ø—Ä–∏–∑–Ω–∞–∫, –Ω–µ –∑–∞–¥–µ–≤ –æ—Å—Ç–∞–ª—å–Ω—ã–µ.\n",
    "\n",
    "–ß—Ç–æ–±—ã –∑–∞—Å—Ç–∞–≤–∏—Ç—å —Å–µ—Ç—å –≤—ã—É—á–∏—Ç—å –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ —Å—Ç–∏–ª–∏, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ç—Ä—é–∫ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:\n",
    "\n",
    "- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–≤–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–∞ $\\mathbf{z}_1$ –∏ $\\mathbf{z}_2$\n",
    "\n",
    "- –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Ö —á–µ—Ä–µ–∑ `Mapping Network` –∏ –ø–æ–ª—É—á–∞–µ–º $\\mathbf{w}_1$ –∏ $\\mathbf{w}_2$\n",
    "\n",
    "- –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –Ω–æ–º–µ—Ä —Å–ª–æ—è –≤ —Å–µ—Ç–∏ —Å–∏–Ω—Ç–µ–∑–∞.\n",
    "\n",
    "- –î–ª—è —Å–ª–æ–µ–≤ –¥–æ —ç—Ç–æ–≥–æ –Ω–æ–º–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º $\\mathbf{w}_1$, –∞ –ø–æ—Å–ª–µ ‚Äî $\\mathbf{w}_2$.\n",
    "\n",
    "–≠—Ç–æ –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ—Ç—å –ø–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ —Å—Ç–∏–ª—å –Ω–∞ —É—Ä–æ–≤–Ω–µ $4\\times 4$ (—Ñ–æ—Ä–º–∞ –ª–∏—Ü–∞) –Ω–µ –¥–æ–ª–∂–µ–Ω –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç —Å—Ç–∏–ª—è –Ω–∞ —É—Ä–æ–≤–Ω–µ $64\\times 64$ (—Ü–≤–µ—Ç –∫–æ–∂–∏).\n",
    "\n",
    "–í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ª–æ–≥–∏–∫—É **Style Mixing** –≤–Ω—É—Ç—Ä–∏ –º–µ—Ç–æ–¥–∞ `forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    –°lass for the StyleGAN Generator.\n",
    "\n",
    "    Args:\n",
    "        resolution: Target output resolution\n",
    "        latent_size: Dimensionality of input latent Z.\n",
    "        dlatent_size: Dimensionality of intermediate latent W.\n",
    "        truncation_psi: Truncation threshold\n",
    "        truncation_cutoff: Number of layers for which truncation is applied.\n",
    "        dlatent_avg_beta: Decay for tracking the moving average of W.\n",
    "        style_mixing_prob: Probability of mixing styles during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 resolution, \n",
    "                 latent_size=512, \n",
    "                 dlatent_size=512,\n",
    "                 truncation_psi=0.7,\n",
    "                 truncation_cutoff=8, \n",
    "                 dlatent_avg_beta=0.995,\n",
    "                 style_mixing_prob=0.9, \n",
    "                 **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.style_mixing_prob = style_mixing_prob\n",
    "\n",
    "        # Calculate total number of layers in the synthesis network\n",
    "        self.num_layers = (int(np.log2(resolution)) - 1) * 2\n",
    "\n",
    "        self.mapping_network = MappingNetwork(\n",
    "            latent_size=latent_size,\n",
    "            dlatent_size=dlatent_size, \n",
    "            num_layers_broadcast=self.num_layers,\n",
    "            **kwargs \n",
    "        )\n",
    "        \n",
    "        self.synthesis_network = SynthesisNetwork(\n",
    "            resolution=resolution, \n",
    "            dlatent_size=dlatent_size,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        if truncation_psi > 0:\n",
    "            self.truncation = Truncation(\n",
    "                avg_latent=torch.zeros(dlatent_size),\n",
    "                max_layer=truncation_cutoff,\n",
    "                threshold=truncation_psi,\n",
    "                beta=dlatent_avg_beta\n",
    "            )\n",
    "        else:\n",
    "            self.truncation = None\n",
    "\n",
    "    def forward(self, latents_in, depth, alpha):\n",
    "        \n",
    "        # Pass input latents through the mapping network\n",
    "        dlatents_in = ...\n",
    "\n",
    "        if self.training:\n",
    "            # Update moving average for truncation\n",
    "            if self.truncation is not None:\n",
    "                 avg_w = dlatents_in[:, 0].mean(dim=0).detach()\n",
    "                 self.truncation.update(avg_w.to(self.truncation.avg_latent.device))\n",
    "\n",
    "            # Style Mixing Regularization\n",
    "            if self.style_mixing_prob is not None and self.style_mixing_prob > 0:\n",
    "                # Create a second random batch of latents\n",
    "                latents2 = torch.randn_like(latents_in)\n",
    "                # Get w vectors for the second batch\n",
    "                dlatents2 = ...\n",
    "\n",
    "                # Create layer indices for masking: [1, Num_Layers, 1]\n",
    "                layer_idx = ...\n",
    "\n",
    "                # Pick a random crossover point\n",
    "                # We mix styles only up to the current depth\n",
    "                cur_layers = 2 * (depth + 1)\n",
    "                \n",
    "                # With probability 'style_mixing_prob', pick a random cutoff\n",
    "                # Otherwise, use all layers (no mixing)\n",
    "                mixing_cutoff = random.randint(1, cur_layers) if random.random() < self.style_mixing_prob else cur_layers\n",
    "                \n",
    "                # Mix dlatents_in and dlatents2 based on layer_idx and mixing_cutoff\n",
    "                # Where layer < cutoff, use dlatents_in, else use dlatents2\n",
    "                dlatents_in = torch.where(..., ..., ...)\n",
    "\n",
    "            # Apply Truncation Trick \n",
    "            if self.truncation is not None:\n",
    "                dlatents_in = ...\n",
    "\n",
    "        # Pass the final w styles to the synthesis network\n",
    "        fake_images = ...\n",
    "\n",
    "        return fake_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88486723",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 14: Discriminator (1 –±–∞–ª–ª)\n",
    "\n",
    "**–î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä** (**–∫—Ä–∏—Ç–∏–∫**) –≤ `StyleGAN` –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ—Å—Ç—å—é –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞.\n",
    "\n",
    "–ï—Å–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–æ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä –≤—ã–ø–æ–ª–Ω—è–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é –æ–ø–µ—Ä–∞—Ü–∏—é. –û–Ω –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –≥–æ—Ç–æ–≤–æ–µ RGB-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —Å–∂–∏–º–∞–µ—Ç –µ–≥–æ (**downsampling**), –ø–æ–∫–∞ –Ω–µ –ø–æ–ª—É—á–∏—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä.\n",
    "\n",
    "–ö–æ–≥–¥–∞ –º—ã –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Å–ª–æ–π –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è, –º—ã —Ç–∞–∫ –∂–µ, –∫–∞–∫ –∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–ª–∞–≤–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ (**Fade-in**) —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º $\\alpha$.\n",
    "\n",
    "–û–±—Ä–∞—Ç–∏—Ç–µ, —á—Ç–æ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º `DiscriminatorOutputBlock`.\n",
    "\n",
    "–í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –≤–∞–º –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f63ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator progressively downsamples the image to a single score\n",
    "\n",
    "    Args:\n",
    "            resolution: Input image resolution\n",
    "            num_channels: Number of input color channels\n",
    "            fmap_base: Base number of feature maps\n",
    "            fmap_decay: Decay rate for feature maps\n",
    "            fmap_max: Maximum number of feature maps\n",
    "            nonlinearity: Activation function name\n",
    "            mbstd_group_size: Group size for Minibatch StdDev layer\n",
    "            mbstd_num_features: Number of features for Minibatch StdDev layer\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 resolution, \n",
    "                 num_channels=3, \n",
    "                 fmap_base=8192, \n",
    "                 fmap_decay=1.0, \n",
    "                 fmap_max=512,\n",
    "                 nonlinearity='lrelu', \n",
    "                 mbstd_group_size=4,\n",
    "                 mbstd_num_features=1, \n",
    "                 **kwargs): \n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.mbstd_num_features = mbstd_num_features\n",
    "        self.mbstd_group_size = mbstd_group_size\n",
    "        self.fmap_base = fmap_base\n",
    "        self.fmap_decay = fmap_decay\n",
    "        self.fmap_max = fmap_max\n",
    "\n",
    "        resolution_log2 = int(np.log2(resolution))\n",
    "        assert resolution == 2 ** resolution_log2 and resolution >= 4\n",
    "        self.depth = resolution_log2 - 1\n",
    "\n",
    "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
    "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
    "\n",
    "        blocks = []\n",
    "        from_rgb = []\n",
    "        \n",
    "        # Loop descending from max resolution down to 8x8\n",
    "        for res in range(resolution_log2, 2, -1):\n",
    "            # Calculate channels for current (in) and next smaller (out) resolution\n",
    "            in_ch = ...\n",
    "            out_ch = ...\n",
    "            \n",
    "            # Append a DiscriminatorBlock (in_ch -> out_ch)\n",
    "            blocks.append(...)\n",
    "    \n",
    "            # Append a FromRGB layer (num_channels -> in_ch)\n",
    "            # This is an EqualizedConv2d with kernel_size=1\n",
    "            from_rgb.append(...)\n",
    "            \n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.from_rgb = nn.ModuleList(from_rgb)\n",
    "\n",
    "        # Initialize DiscriminatorOutputBlock\n",
    "        self.final_block = ...\n",
    "        \n",
    "        # FromRGB for the final 4x4 block\n",
    "        self.from_rgb.append(EqualizedConv2d(\n",
    "            num_channels, self._get_fmaps(2), kernel_size=1, \n",
    "            gain=gain\n",
    "        ))\n",
    "\n",
    "        # Downsampler for the residual path in forward()\n",
    "        self.downsample = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "    def _get_fmaps(self, stage):\n",
    "        return min(\n",
    "            int(self.fmap_base / (2.0 ** (stage * self.fmap_decay))), \n",
    "            self.fmap_max\n",
    "        )\n",
    "\n",
    "    def forward(self, images_in, depth, alpha=1.0):\n",
    "        \n",
    "        assert depth < self.depth\n",
    "\n",
    "        if depth > 0:\n",
    "            # Downsample images_in and apply the OLD FromRGB layer\n",
    "            residual = ...\n",
    "            residual = ...\n",
    "\n",
    "            # Apply NEW FromRGB layer\n",
    "            straight = ...\n",
    "            # Apply the NEW DiscriminatorBlock\n",
    "            straight = ...\n",
    "\n",
    "            # Mix straight and residual using alpha\n",
    "            x = ...\n",
    "\n",
    "            # Run through the remaining stabilized blocks\n",
    "            for block in self.blocks[(self.depth - depth):]:\n",
    "                x = ...\n",
    "        else:\n",
    "            # Apply the last FromRGB layer directly\n",
    "            x = ...\n",
    "        \n",
    "        # Final Scoring\n",
    "        scores_out = ...\n",
    "\n",
    "        return scores_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6dd82",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 15: –§—É–Ω–∫—Ü–∏—è –ü–æ—Ç–µ—Ä—å (0.5 –±–∞–ª–ª–∞)\n",
    "\n",
    "\n",
    "–•–æ—Ç—è –≤ —Ä–∞–Ω–Ω–∏—Ö –≤–µ—Ä—Å–∏—è—Ö –¥–ª—è `CelebA` –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è `WGAN-GP`, –∑–æ–ª–æ—Ç—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º –¥–ª—è `StyleGAN` —Å—Ç–∞–ª–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è **Non-Saturating Logistic Loss —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π R1**. –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç:\n",
    "\n",
    "- **Logistic Loss**: –≠—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–Ω–∞—Ä–Ω–∞—è –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è, –Ω–æ –∑–∞–ø–∏—Å–∞–Ω–Ω–∞—è —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é $\\text{Softplus}(x) = \\log(1 + e^x)$ –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "\n",
    "- **R1 Regularization**: –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç `WGAN-GP`, –∫–æ—Ç–æ—Ä—ã–π —à—Ç—Ä–∞—Ñ—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–µ–∑–¥–µ, R1 –Ω–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç —à—Ç—Ä–∞—Ñ —Ç–æ–ª—å–∫–æ –Ω–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞ –≤ —Ç–æ—á–∫–∞—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\n",
    "\n",
    "$$R_1 = \\frac{\\gamma}{2} \\mathbb{E}_{\\mathbf{x} \\sim p_{data}(\\mathbf{x})} [ ||\\nabla D(\\mathbf{x})||^2 ]$$\n",
    "\n",
    "–í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å `LogisticGAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd697d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticGAN(nn.Module):\n",
    "    def __init__(self, discriminator):\n",
    "        super().__init__()\n",
    "        self.dis = discriminator\n",
    "\n",
    "    def R1Penalty(self, real_img, height, alpha):\n",
    "\n",
    "        # Enable gradient tracking for the input image\n",
    "        # We need this because we are calculating the gradient w.r.t. the IMAGE, not weights.\n",
    "        real_img = real_img.detach().requires_grad_(True)\n",
    "        \n",
    "        # Get Discriminator output (logits)\n",
    "        real_logit = ...\n",
    "        \n",
    "        # Calculate Gradients \n",
    "        real_grads = torch.autograd.grad(\n",
    "            outputs=..., \n",
    "            inputs=..., \n",
    "            grad_outputs=torch.ones_like(real_logit),\n",
    "            create_graph=True, \n",
    "            retain_graph=True\n",
    "        )[0].view(real_img.size(0), -1)\n",
    "        \n",
    "        # Calculate Penalty: sum(grads^2) across pixels, then mean across batch\n",
    "        r1_penalty = ...\n",
    "        \n",
    "        return r1_penalty\n",
    "\n",
    "    def dis_loss(self, real_samps, fake_samps, height, alpha, r1_gamma=10.0):\n",
    "\n",
    "        r_preds = self.dis(real_samps, height, alpha)\n",
    "        f_preds = self.dis(fake_samps, height, alpha)\n",
    "\n",
    "        # Implement the logistic loss formula for D\n",
    "        loss = ...\n",
    "\n",
    "        # R1 Regularization\n",
    "        if r1_gamma > 0.0:\n",
    "            r1_penalty = self.R1Penalty(real_samps.detach(), height, alpha)\n",
    "            # Add weighted penalty to loss\n",
    "            loss += ...\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def gen_loss(self, _, fake_samps, height, alpha):\n",
    "        f_preds = self.dis(fake_samps, height, alpha)\n",
    "        \n",
    "        # Implement the non-saturating generator loss\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a4cafc",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –ø–µ—Ä–µ—Ö–æ–¥–∏–º –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∫ –æ–±—É—á–µ–Ω–∏—é. –ß—Ç–æ–±—ã –≤—ã –º–æ–≥–ª–∏ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ —Å–∞–º–æ–π —Å—É—Ç–∏ `StyleGAN`, –∫–æ–¥ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏, –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∑–∞ –≤–∞—Å.\n",
    "\n",
    "–§—É–Ω–∫—Ü–∏–∏ `plot_loss_curves` –∏ `plot_sample_grid` —Å—Ç—Ä–æ—è—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –ª–æ—Å—Å–æ–≤ –∏ —Å–µ—Ç–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.\n",
    "\n",
    "–§—É–Ω–∫–∏—è `update_average` —Ä–µ–∞–ª–∏–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º **Exponential Moving Average** (**EMA**). –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤–µ–¥–µ—Ç —Å–µ–±—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ, –µ–≥–æ –≤–µ—Å–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –∏–∑–º–µ–Ω—è—é—Ç—Å—è. –ß—Ç–æ–±—ã –ø–æ–ª—É—á–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º **\"—Ç–µ–Ω–µ–≤—É—é\" –∫–æ–ø–∏—é –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞** (`gen_shadow`), –≤–µ—Å–∞ –∫–æ—Ç–æ—Ä–æ–π –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –æ—á–µ–Ω—å –ø–ª–∞–≤–Ω–æ –∏ —É—Å—Ä–µ–¥–Ω—è—é—Ç—Å—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏. –ò–º–µ–Ω–Ω–æ —ç—Ç—É –≤–µ—Ä—Å–∏—é –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.\n",
    "\n",
    "–í—Å—é –ª–æ–≥–∏–∫—É –æ–±—É—á–µ–Ω–∏—è –º—ã –¥–æ–±–∞–≤–∏–ª–∏ –≤ –∫–ª–∞—Å—Å `StyleGANTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(train_d_losses, train_g_losses):\n",
    "  \n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(train_d_losses, label='D Loss', alpha=0.7, linewidth=1)\n",
    "    plt.plot(train_g_losses, label='G Loss', alpha=0.7, linewidth=1)\n",
    "    \n",
    "    plt.title(\"Training Losses\")\n",
    "    plt.xlabel(\"Batch Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10101a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_grid(generator, fixed_noise, depth, alpha, gen_shadow=None, \n",
    "                     is_final_for_depth=False):\n",
    "\n",
    "    model = gen_shadow\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images = model(fixed_noise, depth, alpha).detach().cpu()\n",
    "    \n",
    "    current_res = 2 ** (depth + 2)\n",
    "    \n",
    "    if images.shape[2] < 256:\n",
    "        images = F.interpolate(images, size=256, mode='nearest')\n",
    "\n",
    "    images = (images * 0.5 + 0.5).clamp(0, 1)\n",
    "    \n",
    "    grid = make_grid(images, nrow=len(images), padding=2, pad_value=1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.imshow(grid.permute(1, 2, 0).numpy())\n",
    "    plt.title(f\"Res: {current_res}x{current_res} | Depth: {depth} | Alpha: {alpha:.2f}\" + \n",
    "              (\" FINAL\" if is_final_for_depth else \"\"))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32371256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_average(model_tgt, model_src, beta):\n",
    "    with torch.no_grad():\n",
    "        params_src = dict(model_src.named_parameters())\n",
    "        params_tgt = dict(model_tgt.named_parameters())\n",
    "        for k in params_src.keys():\n",
    "            params_tgt[k].data.copy_(\n",
    "                torch.lerp(params_tgt[k].data, params_src[k].data, 1.0 - beta)\n",
    "            )\n",
    "            \n",
    "class StyleGANTrainer:\n",
    "    def __init__(self, \n",
    "                 resolution, \n",
    "                 num_channels, \n",
    "                 latent_size,\n",
    "                 g_args,                   \n",
    "                 d_args,                   \n",
    "                 generator_optimizer,\n",
    "                 discriminator_optimizer,\n",
    "                 loss_function,\n",
    "                 d_repeats=1,\n",
    "                 ema_decay=0.999, \n",
    "                 device=None):\n",
    "        \n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.depth = int(np.log2(resolution)) - 1\n",
    "        self.latent_size = latent_size\n",
    "        self.d_repeats = d_repeats\n",
    "        self.ema_decay = ema_decay\n",
    "        \n",
    "        self.gen = Generator(\n",
    "            resolution=resolution, \n",
    "            latent_size=latent_size,\n",
    "            **g_args).to(self.device)\n",
    "\n",
    "        self.dis = Discriminator(\n",
    "            resolution=resolution, \n",
    "            num_channels=num_channels,\n",
    "            **d_args).to(self.device)\n",
    "\n",
    "        self.gen_optim = generator_optimizer\n",
    "        self.dis_optim = discriminator_optimizer\n",
    "        self.loss = loss_function\n",
    "        \n",
    "        self.gen_shadow = copy.deepcopy(self.gen).eval()\n",
    "        self._update_ema(beta=0.0) \n",
    "\n",
    "        self.history = {'d_loss': [], 'g_loss': []}\n",
    "        self.final_grids_history = [] \n",
    "\n",
    "    def _update_ema(self, beta):\n",
    "        update_average(self.gen_shadow, self.gen, beta)\n",
    "\n",
    "    def _progressive_downsampling(self, real_batch, depth, alpha):\n",
    "        target_res = 2 ** (depth + 2)\n",
    "        target_scale = target_res / real_batch.shape[2]\n",
    "        ds_real_samples = F.interpolate(real_batch, scale_factor=target_scale, mode='area')\n",
    "        \n",
    "        if depth > 0 and alpha < 1.0:\n",
    "            prior_res = 2 ** (depth + 1)\n",
    "            prior_scale = prior_res / real_batch.shape[2]\n",
    "            prior_ds_real_samples = F.interpolate(real_batch, scale_factor=prior_scale, mode='area')\n",
    "            prior_ds_real_samples = F.interpolate(prior_ds_real_samples, scale_factor=2, mode='nearest')\n",
    "            real_samples = (alpha * ds_real_samples) + ((1 - alpha) * prior_ds_real_samples)\n",
    "        else:\n",
    "            real_samples = ds_real_samples\n",
    "        return real_samples.to(self.device)\n",
    "\n",
    "    def _train_step_discriminator(self, noise, real_batch, depth, alpha):\n",
    "        self.gen.eval()\n",
    "        self.dis.train()\n",
    "        loss_val = 0\n",
    "        for _ in range(self.d_repeats):\n",
    "            real_samples_ds = self._progressive_downsampling(real_batch, depth, alpha)\n",
    "            with torch.no_grad():\n",
    "                fake_samples = self.gen(noise, depth, alpha).detach()\n",
    "            \n",
    "            loss = self.loss.dis_loss(real_samples_ds, fake_samples, depth, alpha)\n",
    "            \n",
    "            self.dis_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.dis_optim.step()\n",
    "            loss_val += loss.item()\n",
    "        return loss_val / self.d_repeats\n",
    "\n",
    "    def _train_step_generator(self, noise, real_batch, depth, alpha):\n",
    "        self.gen.train()\n",
    "        self.dis.eval()\n",
    "        real_samples_ds = self._progressive_downsampling(real_batch, depth, alpha)\n",
    "        fake_samples = self.gen(noise, depth, alpha)\n",
    "        \n",
    "        loss = self.loss.gen_loss(real_samples_ds, fake_samples, depth, alpha)\n",
    "        \n",
    "        self.gen_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient Clipping\n",
    "        nn.utils.clip_grad_norm_(self.gen.parameters(), max_norm=10.)\n",
    "        self.gen_optim.step()\n",
    "        \n",
    "        self._update_ema(beta=self.ema_decay)\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, \n",
    "              train_loader, \n",
    "              epochs_per_depth, \n",
    "              fade_in_percentage, \n",
    "              output_dir, \n",
    "              num_samples=5, \n",
    "              start_depth=0, \n",
    "              checkpoint_factor=1):\n",
    "        \n",
    "        print(\"Starting the training process...\")\n",
    "        \n",
    "        self.gen.train()\n",
    "        self.dis.train()\n",
    "        self.gen_shadow.train() \n",
    "            \n",
    "        fixed_noise = torch.randn(num_samples, self.latent_size).to(self.device)\n",
    "        \n",
    "        for current_depth in range(start_depth, self.depth):\n",
    "            current_res = 2 ** (current_depth + 2)\n",
    "            print(f\"\\n Training depth: {current_depth} ({current_res}x{current_res}) \")\n",
    "            \n",
    "            total_batches = len(train_loader)\n",
    "            ticker = 1\n",
    "            current_epochs = epochs_per_depth[current_depth]\n",
    "            current_fade_in = fade_in_percentage[current_depth]\n",
    "            \n",
    "            for epoch in range(1, current_epochs + 1):\n",
    "                fade_point = int((current_fade_in / 100) * current_epochs * total_batches)\n",
    "                \n",
    "                pbar = tqdm(train_loader, initial=0, leave=True, unit=\"batch\")\n",
    "                pbar.set_description(f\"Depth {current_depth} | Epoch {epoch}/{current_epochs}\")\n",
    "\n",
    "                for i, batch_data in enumerate(pbar, 1):\n",
    "                    real_batch = batch_data[0] if isinstance(batch_data, list) else batch_data\n",
    "                    \n",
    "                    alpha = ticker / fade_point if ticker <= fade_point else 1.0\n",
    "                    \n",
    "                    real_batch = real_batch.to(self.device)\n",
    "                    noise = torch.randn(real_batch.size(0), self.latent_size).to(self.device)\n",
    "                    \n",
    "                    dis_loss = self._train_step_discriminator(noise, real_batch, current_depth, alpha)\n",
    "                    gen_loss = self._train_step_generator(noise, real_batch, current_depth, alpha)\n",
    "                    \n",
    "                    self.history['d_loss'].append(dis_loss)\n",
    "                    self.history['g_loss'].append(gen_loss)\n",
    "\n",
    "                    pbar.set_postfix({\n",
    "                        'G': f'{gen_loss:.3f}',\n",
    "                        'D': f'{dis_loss:.3f}',\n",
    "                        'Alpha': f'{alpha:.2f}'\n",
    "                    })\n",
    "                    \n",
    "                    ticker += 1\n",
    "                \n",
    "                print(f\"Epoch {epoch} Done.\")\n",
    "                \n",
    "                plot_loss_curves(\n",
    "                    self.history['d_loss'], \n",
    "                    self.history['g_loss'], \n",
    "                )\n",
    "\n",
    "                if self.final_grids_history:\n",
    "                    print(\"History of previous depths\")\n",
    "                    for i, (saved_grid, saved_res) in enumerate(self.final_grids_history):\n",
    "                        plt.figure(figsize=(15, 2))\n",
    "                        plt.imshow(saved_grid.permute(1, 2, 0).numpy())\n",
    "                        plt.title(f\"Completed Depth {i} ({saved_res}x{saved_res})\")\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "\n",
    "                is_last_epoch = (epoch == current_epochs)\n",
    "                print(f\"Current Depth {current_depth}\")\n",
    "                plot_sample_grid(\n",
    "                    self.gen, fixed_noise, current_depth, alpha, \n",
    "                    use_ema=True, gen_shadow=self.gen_shadow,\n",
    "                    is_final_for_depth=is_last_epoch\n",
    "                )\n",
    "                \n",
    "                if is_last_epoch:\n",
    "                    with torch.no_grad():\n",
    "                        self.gen_shadow.eval()\n",
    "                        imgs = self.gen_shadow(fixed_noise, current_depth, alpha).detach().cpu()\n",
    "                        if imgs.shape[2] < 256: \n",
    "                            imgs = F.interpolate(imgs, size=256, mode='nearest')\n",
    "                        imgs = (imgs * 0.5 + 0.5).clamp(0, 1)\n",
    "                        grid = make_grid(imgs, nrow=len(imgs), padding=2, pad_value=1)\n",
    "                        self.final_grids_history.append((grid, 2**(current_depth+2)))\n",
    "\n",
    "                if epoch % checkpoint_factor == 0 or is_last_epoch:\n",
    "                    self.save_checkpoint(output_dir, current_depth, epoch)\n",
    "                        \n",
    "        print(\"Training completed!\")\n",
    "\n",
    "    def save_checkpoint(self, output_dir, depth, epoch):\n",
    "        save_dir = os.path.join(output_dir, 'models')\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        suffix = f\"depth_{depth}_epoch_{epoch}.pth\"\n",
    "        torch.save(self.gen.state_dict(), os.path.join(save_dir, f\"gen_{suffix}\"))\n",
    "        torch.save(self.dis.state_dict(), os.path.join(save_dir, f\"dis_{suffix}\"))\n",
    "        torch.save(self.gen_optim.state_dict(), os.path.join(save_dir, f\"gen_optim_{suffix}\"))\n",
    "        torch.save(self.dis_optim.state_dict(), os.path.join(save_dir, f\"dis_optim_{suffix}\"))\n",
    "        torch.save(self.gen_shadow.state_dict(), os.path.join(save_dir, f\"gen_shadow_{suffix}\"))\n",
    "        \n",
    "        print(f\"Checkpoint is saved: {save_dir}/...{suffix}\")\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_dir, depth, epoch, load_optim=True):\n",
    "        print(f\"Loading checkpoint from: {checkpoint_dir}\")\n",
    "        suffix = f\"depth_{depth}_epoch_{epoch}.pth\"\n",
    "        \n",
    "        self.gen.load_state_dict(torch.load(os.path.join(checkpoint_dir, f\"gen_{suffix}\"), map_location=self.device))\n",
    "        self.dis.load_state_dict(torch.load(os.path.join(checkpoint_dir, f\"dis_{suffix}\"), map_location=self.device))\n",
    "        \n",
    "        if load_optim:\n",
    "            try:\n",
    "                self.gen_optim.load_state_dict(torch.load(os.path.join(checkpoint_dir, f\"gen_optim_{suffix}\"), map_location=self.device))\n",
    "                self.dis_optim.load_state_dict(torch.load(os.path.join(checkpoint_dir, f\"dis_optim_{suffix}\"), map_location=self.device))\n",
    "            except FileNotFoundError:\n",
    "                print(\"The optimizer files were not found. Skip the loading.\")\n",
    "        \n",
    "\n",
    "        self.gen_shadow.load_state_dict(torch.load(os.path.join(checkpoint_dir, f\"gen_shadow_{suffix}\"), map_location=self.device))\n",
    "        \n",
    "        print(\"Checkpoint was uploaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5127cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 128\n",
    "LATENT_SIZE = 512\n",
    "D_LATENT_SIZE = 512\n",
    "NUM_CHANNELS = 3\n",
    "\n",
    "EPOCHS_PER_DEPTH = [4, 8, 12, 16, 24, 36] \n",
    "FADE_IN_PERCENTAGE = [50, 50, 50, 50, 50, 50]\n",
    "\n",
    "G_LR = 2e-3\n",
    "D_LR = 2e-3\n",
    "BETAS = (0.0, 0.99)\n",
    "D_REPEATS = 1\n",
    "EMA_DECAY = 0.999\n",
    "\n",
    "g_args = {\n",
    "    'mapping_layers': 8,\n",
    "    'fmap_base': 8192,\n",
    "    'fmap_decay': 1.0,\n",
    "    'fmap_max': 512,\n",
    "}\n",
    "d_args = {\n",
    "    'mbstd_group_size': 4,\n",
    "    'mbstd_num_features': 1,\n",
    "    'fmap_base': 8192,\n",
    "    'fmap_decay': 1.0,\n",
    "    'fmap_max': 512,\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = \"stylegan_checkpoints\"\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01836e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(RESOLUTION, LATENT_SIZE, D_LATENT_SIZE, **g_args)\n",
    "dis = Discriminator(RESOLUTION, NUM_CHANNELS, **d_args)\n",
    "\n",
    "g_optim = optim.Adam(gen.parameters(), lr=G_LR, betas=BETAS)\n",
    "d_optim = optim.Adam(dis.parameters(), lr=D_LR, betas=BETAS)\n",
    "\n",
    "loss_fn = LogisticGAN(dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cb87f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = StyleGANTrainer(\n",
    "    resolution=RESOLUTION,\n",
    "    num_channels=NUM_CHANNELS,\n",
    "    latent_size=LATENT_SIZE,\n",
    "    g_args=g_args,\n",
    "    d_args=d_args,\n",
    "    \n",
    "    generator_optimizer=g_optim,\n",
    "    discriminator_optimizer=d_optim,\n",
    "    loss_function=loss_fn,\n",
    "    \n",
    "    d_repeats=D_REPEATS,\n",
    "    ema_decay=EMA_DECAY,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "trainer.gen_optim.param_groups[0]['params'] = list(trainer.gen.parameters())\n",
    "trainer.dis_optim.param_groups[0]['params'] = list(trainer.dis.parameters())\n",
    "trainer.loss.dis = trainer.dis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccada520",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    train_loader=train_loader,  \n",
    "    epochs_per_depth=EPOCHS_PER_DEPTH,\n",
    "    fade_in_percentage=FADE_IN_PERCENTAGE,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_samples=5,\n",
    "    start_depth=0,\n",
    "    checkpoint_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae40af",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 16, –±–æ–Ω—É—Å: Style Mixing (1 –±–∞–ª–ª)\n",
    "\n",
    "–û–¥–Ω–æ–π –∏–∑ —Å–∞–º—ã—Ö –≥–ª–∞–≤–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π `StyleGAN` —è–≤–ª—è–µ—Ç—Å—è **—Å–º–µ—à–∏–≤–∞–Ω–∏–µ —Å—Ç–∏–ª–µ–π** (**Style Mixing**). –ü–æ—Å–∫–æ–ª—å–∫—É –º—ã –∑–Ω–∞–µ–º, —á—Ç–æ —Ä–∞–∑–Ω—ã–µ —Å–ª–æ–∏ —Å–µ—Ç–∏ —Å–∏–Ω—Ç–µ–∑–∞ –æ—Ç–≤–µ—á–∞—é—Ç –∑–∞ —Ä–∞–∑–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏, –º—ã –º–æ–∂–µ–º —Å–æ–∑–¥–∞–≤–∞—Ç—å –Ω–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∫–æ–º–±–∏–Ω–∏—Ä—É—è –≤–µ–∫—Ç–æ—Ä—ã —Å—Ç–∏–ª—è $\\mathbf{w}$ –æ—Ç –¥–≤—É—Ö —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.\n",
    "\n",
    "–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å—Ç—Ä–æ–∏—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n",
    "\n",
    "- **Source A**: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä $\\mathbf{w}_A$ –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ—Å–Ω–æ–≤—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n",
    "\n",
    "- **Source B**: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä $\\mathbf{w}_B$ –∏ –≤–Ω–æ—Å–∏—Ç —Å–≤–æ–∏ –∞—Ç—Ä–∏–±—É—Ç—ã.\n",
    "\n",
    "–ú—ã —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π, —Å–º–µ—à–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä $\\mathbf{w}_{mix}$, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ $\\mathbf{w}_A$, –Ω–æ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å–ª–æ—è –º—ã –∑–∞–º–µ–Ω—è–µ–º –µ–≥–æ –Ω–∞ $\\mathbf{w}_B$.\n",
    "\n",
    "–£—Ä–æ–≤–Ω–∏ —Å–º–µ—à–∏–≤–∞–Ω–∏—è:\n",
    "\n",
    "- **Coarse**: –ó–∞–º–µ–Ω–∞ $\\mathbf{w}_A$ –Ω–∞ $\\mathbf{w}_B$ –≤ —Å–ª–æ—è—Ö –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è ($4 \\times 4 - 8 \\times 8$)\n",
    "\n",
    "- **Middle**: –ó–∞–º–µ–Ω–∞ –≤ —Å–ª–æ—è—Ö —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è ($16 \\times 16 - 32 \\times 32$)\n",
    "\n",
    "- **Fine**: –ó–∞–º–µ–Ω–∞ –≤ —Å–ª–æ—è—Ö –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è ($64 \\times 64 - 128 \\times 128$)\n",
    "\n",
    "–í–∞–º –Ω—É–∂–Ω–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –≤ —Ñ—É–Ω–∫—Ü–∏—è—Ö `mix_styles` –∏ `visualize_style_mixing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fa2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_styles(w_batch_A, w_batch_B, style_range):\n",
    "    \"\"\"\n",
    "    Creates a mixed W tensor by copying styles from B into A for specified layers.\n",
    "    Args:\n",
    "        w_batch_A: W tensor from Source A\n",
    "        w_batch_B: W tensor from Source B\n",
    "        style_range: List or range of layer indices to swap \n",
    "    \"\"\"\n",
    "    w_mix = w_batch_A.clone()\n",
    "    \n",
    "    # Perform the style swap using slicing\n",
    "    w_mix[:, ..., :] = ...\n",
    "    \n",
    "    return w_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29770afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_style_mixing(generator, \n",
    "                           n_source_A=5, \n",
    "                           n_source_B=3,\n",
    "                           style_type='coarse',\n",
    "                           device='cuda',\n",
    "                           seed=42):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if style_type == 'coarse':\n",
    "        style_layers = range(0, 4)   # 4x4, 8x8\n",
    "    elif style_type == 'middle':\n",
    "        style_layers = range(4, 8)   # 16x16, 32x32\n",
    "    elif style_type == 'fine':\n",
    "        style_layers = range(8, 12)  # 64x64, 128x128\n",
    "        \n",
    "    print(f\"Generating mixing grid for '{style_type}' styles (layers {list(style_layers)})...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dim_z = generator.mapping_network.layers[0].weight.shape[1]\n",
    "        z_A = torch.randn(n_source_A, dim_z).to(device)\n",
    "        z_B = torch.randn(n_source_B, dim_z).to(device)\n",
    "        \n",
    "        # Get w using Mapping Network\n",
    "        w_A =...\n",
    "        w_B = ...\n",
    "        \n",
    "\n",
    "        max_depth = generator.synthesis_network.depth - 1\n",
    "        \n",
    "        # Get images using Synthesis Network\n",
    "        img_A = ...\n",
    "        img_B = ...\n",
    "\n",
    "        rows = []\n",
    "\n",
    "        empty = torch.ones(1, 3, 128, 128).to(device) \n",
    "        header_row = torch.cat([empty, img_A], dim=0)\n",
    "        rows.append(header_row)\n",
    "        \n",
    "        for i in range(n_source_B):\n",
    "            w_this_B = w_B[i].unsqueeze(0).expand(n_source_A, -1, -1)\n",
    "            \n",
    "            w_mixed = mix_styles(generator, w_batch_A=w_A, w_batch_B=w_this_B, style_range=style_layers)\n",
    "            img_mixed = generator.synthesis_network(w_mixed, depth=max_depth, alpha=1.0)\n",
    "            \n",
    "            row = torch.cat([img_B[i].unsqueeze(0), img_mixed], dim=0)\n",
    "            rows.append(row)\n",
    "            \n",
    "        final_grid_tensor = torch.cat(rows, dim=0)\n",
    "        \n",
    "        final_grid_tensor = (final_grid_tensor * 0.5 + 0.5).clamp(0, 1)\n",
    "\n",
    "        grid_img = make_grid(final_grid_tensor, nrow=n_source_A + 1, padding=2, pad_value=1)\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.imshow(grid_img.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Style Mixing: {style_type.capitalize()} Styles\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b91c43",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–º–µ—à–∏–≤–∞–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö: `coarse`, `middle` –∏ `fine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#‚ï∞( Õ°¬∞ Õú ñ Õ°¬∞ )„Å§‚îÄ‚îÄ‚òÜ*:„ÉªÔæü \n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –æ—Ç–∑—ã–≤—ã, –ø–æ–∂–µ–ª–∞–Ω–∏—è –∏ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏—è –æ –î–ó :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nick_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
